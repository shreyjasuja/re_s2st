{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provision a GPU/CPU Server on Chameleon and Connect to Colab\n",
    "\n",
    "This notebook describes how to connect Colab to a server running on Chameleon. This allows you to run experiments requiring bare metal access, storage, memory, GPU and compute that exceeds the abilities of Colab's hosted runtime, but with Colab's familiar interface (and notebooks stored in your Google Drive). It also allows you to easily go back and forth between the convenience of Colab's hosted runtime and Chameleon's greater capabilities, depending on the needs of your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision the resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to reserve a bare metal Ubuntu server with RTX6000 GPU on CHI@UC - pending availability. Before you begin, you should check the host calendar at [https://chi.uc.chameleoncloud.org/project/leases/calendar/host/](https://chi.uc.chameleoncloud.org/project/leases/calendar/host/). In the \"Node Type\" dropdown, filter on `gpu_rtx_6000` for GPU or `skylake` for CPU and make sure some hosts are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chameleon configuration\n",
    "\n",
    "You can change your Chameleon project name (if not using the one that is automatically configured in the JupyterHub environment) and the site on which to reserve resources (depending on availability) in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using CHI@UC:\n",
      "URL: https://chi.uc.chameleoncloud.org\n",
      "Location: Argonne National Laboratory, Lemont, Illinois, USA\n",
      "Support contact: help@chameleoncloud.org\n"
     ]
    }
   ],
   "source": [
    "import chi, os, time\n",
    "from chi import lease\n",
    "from chi import server\n",
    "\n",
    "PROJECT_NAME = os.getenv('OS_PROJECT_NAME') # change this if you need to\n",
    "chi.use_site(\"CHI@UC\")\n",
    "chi.set(\"project_name\", PROJECT_NAME)\n",
    "username = os.getenv('USER') # all exp resources will have this prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to change the details of the Chameleon server, e.g. use a different GPU type depending on availability, you can do that in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_TYPE=\"gpu_rtx_6000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservation\n",
    "\n",
    "The following cell will create a reservation that begins now, and ends in 8 hours. You can modify the start and end date as needed. \n",
    "\n",
    "If there is no compatible device available right now, you can refer to the [CHI@UC host calendar](https://chi.uc.chameleoncloud.org/project/leases/calendar/host/) (change the \"Node type\" selection to `gpu_rtx_6000`). Set the start and end time according to availability. Then,\n",
    "\n",
    "* run the cell below to reserve your GPU node for some time in the future\n",
    "* at the beginning of your (future) reservation, you'll run this notebook again but you will *skip* the \"Reservation\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "lease.add_node_reservation(res, node_type=NODE_TYPE, count=1)\n",
    "\n",
    "start_date, end_date = lease.lease_duration(days=2, hours=0)\n",
    "# if you won't start right now - comment the line above, uncomment two lines below\n",
    "# start_date = \"2023-01-01 00:00\" # manually define to desired start time \n",
    "# end_date =   \"2023-01-01 08:00\" # manually define to desired start time \n",
    "\n",
    "l = lease.create_lease(f\"colab-{username}-{NODE_TYPE}-v1\", res, start_date=start_date, end_date=end_date)\n",
    "l = lease.wait_for_active(l[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Provisioning resources\n",
    "\n",
    "This section provisions resources. It will take approximately 10 minutes. You can check on its status in the Chameleon web-based UI: [https://chi.uc.chameleoncloud.org/project/instances/](https://chi.uc.chameleoncloud.org/project/instances/), then come back here when it is in the READY state.  \n",
    "\n",
    "*If you have already provisioned resources, jump to this [section](#server_configured)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provision a new server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lease.get_lease(f\"colab-{username}-{NODE_TYPE}-v3\")\n",
    "reservation_id = lease.get_node_reservation(l[\"id\"])\n",
    "server.create_server(\n",
    "    f\"colab-{username}-{NODE_TYPE}-v3\", \n",
    "    reservation_id=reservation_id,\n",
    "    image_name=\"CC-Ubuntu22.04\"\n",
    ")\n",
    "server_id = server.get_server_id(f\"colab-{username}-{NODE_TYPE}-v3\")\n",
    "server.wait_for_active(server_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate an IP address with this server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_fip = server.associate_floating_ip(server_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and wait for it to come up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.wait_for_tcp(reserved_fip, port=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to this if you have already configured server instances. Otherwise skip this <a id = \"server_configured\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue here, to access already created server\n",
    "l = lease.get_lease(f\"colab-{username}-{NODE_TYPE}-v3\")\n",
    "reservation_id = lease.get_node_reservation(l[\"id\"])\n",
    "server_id = server.get_server_id(f\"colab-{username}-{NODE_TYPE}-v3\")\n",
    "server.wait_for_active(server_id)\n",
    "reserved_fip = [d['addr'] for d in chi.server.show_server(server_id).addresses['sharednet1'] if d['OS-EXT-IPS:type']=='floating'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will install some basic packages in order to connect your Colab frontend to your Chameleon server. However, you may want to log in to your Chameleon server in order to access its terminal and install or configure packages outside of Colab. There is an additional section called **GPU installations** which is supposed to be executed only if GPU server is initiated.\n",
    "\n",
    "To log in to the resource, use File > New > Terminal in the Chameleon JupyterHub environment, or your local terminal, and run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh cc@192.5.86.150\n"
     ]
    }
   ],
   "source": [
    "print(\"ssh cc@\" + reserved_fip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, install an updated CUDA, Python and JupyterHub on your resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chi import ssh\n",
    "\n",
    "node = ssh.Remote(reserved_fip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run('sudo apt update')\n",
    "node.run('sudo apt -y install python3-pip python3-dev')\n",
    "node.run('sudo pip3 install --upgrade pip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPU installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run('wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb')\n",
    "node.run('sudo dpkg -i cuda-keyring_1.0-1_all.deb')\n",
    "node.run('sudo apt update')\n",
    "node.run('sudo apt -y install linux-headers-$(uname -r)')\n",
    "node.run('sudo apt -y install nvidia-driver-545') \n",
    "try:\n",
    "    node.run('sudo reboot') # reboot and wait for it to come up\n",
    "except:\n",
    "    pass\n",
    "server.wait_for_tcp(reserved_fip, port=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = ssh.Remote(reserved_fip) \n",
    "node.run('sudo apt -y install cuda-12-2 cuda-runtime-12-2 cuda-drivers=545.23.08-1')\n",
    "node.run('sudo apt -y install nvidia-gds-12-2') # install instructions say to do this separately!\n",
    "node.run('sudo apt -y install libcudnn8=8.9.6.50-1+cuda12.2 cuda-toolkit-12-2') # make sure the get cuda-11-8 version\n",
    "node.run(\"echo 'PATH=\\\"/usr/local/cuda-12.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/home/cc/.local/bin\\\"' | sudo tee /etc/environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reboot, and make sure we have the specified CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    node.run('sudo reboot')\n",
    "except:\n",
    "    pass\n",
    "server.wait_for_tcp(reserved_fip, port=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = ssh.Remote(reserved_fip) # note: need a new SSH session to get new PATH\n",
    "node.run('nvidia-smi')\n",
    "node.run('nvcc --version')\n",
    "node.run('sudo apt-get -y install ffmpeg')\n",
    "node.run('!sudo apt-get install unzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run('echo $PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add SSH Keys to the new server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add all SSH keys in your account at this site - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova=chi.clients.nova()\n",
    "# iterate over all keypairs in this account\n",
    "for kp in nova.keypairs.list(): \n",
    "    public_key = nova.keypairs.get(kp.name).public_key \n",
    "    node.run(f\"echo {public_key} >> ~/.ssh/authorized_keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python packages installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-20 01:19:43--  https://raw.githubusercontent.com/shreyjasuja/re_s2st/main/requirements.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 193 [text/plain]\n",
      "Saving to: ‘requirements.txt’\n",
      "\n",
      "     0K                                                       100% 7.21M=0s\n",
      "\n",
      "2024-04-20 01:19:43 (7.21 MB/s) - ‘requirements.txt’ saved [193/193]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets==2.17.1 (from -r requirements.txt (line 1))\n",
      "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting python-swiftclient==4.4.0 (from -r requirements.txt (line 2))\n",
      "  Downloading python_swiftclient-4.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting openai-whisper==20231117 (from -r requirements.txt (line 3))\n",
      "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 798.6/798.6 kB 7.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting sacrebleu==2.4.0 (from -r requirements.txt (line 4))\n",
      "  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.4/57.4 kB 9.5 MB/s eta 0:00:00\n",
      "Collecting librosa==0.10.1 (from -r requirements.txt (line 5))\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting soundfile==0.12.1 (from -r requirements.txt (line 6))\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Collecting transformers==4.38.0 (from -r requirements.txt (line 7))\n",
      "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.1/131.1 kB 20.5 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.2.0 (from -r requirements.txt (line 8))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting nltk==3.8.1 (from -r requirements.txt (line 9))\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pycountry==23.12.11 (from -r requirements.txt (line 10))\n",
      "  Downloading pycountry-23.12.11-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy>=1.17 (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=12.0.0 (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets==2.17.1->-r requirements.txt (line 1)) (2.25.1)\n",
      "Collecting tqdm>=4.62.1 (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting xxhash (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting packaging (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets==2.17.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Collecting triton<3,>=2.0.0 (from openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting numba (from openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting torch (from openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper==20231117->-r requirements.txt (line 3)) (8.10.0)\n",
      "Collecting tiktoken (from openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting portalocker (from sacrebleu==2.4.0->-r requirements.txt (line 4))\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting regex (from sacrebleu==2.4.0->-r requirements.txt (line 4))\n",
      "  Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu==2.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu==2.4.0->-r requirements.txt (line 4)) (0.4.4)\n",
      "Collecting lxml (from sacrebleu==2.4.0->-r requirements.txt (line 4))\n",
      "  Downloading lxml-5.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting scipy>=1.2.0 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting scikit-learn>=0.20.0 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=0.14 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting decorator>=4.3.0 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pooch>=1.0 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting typing-extensions>=4.1.1 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting lazy-loader>=0.1 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting cffi>=1.0 (from soundfile==0.12.1->-r requirements.txt (line 6))\n",
      "  Downloading cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0->-r requirements.txt (line 7))\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.0->-r requirements.txt (line 7))\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk==3.8.1->-r requirements.txt (line 9)) (8.0.3)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile==0.12.1->-r requirements.txt (line 6))\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets==2.17.1->-r requirements.txt (line 1)) (21.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting platformdirs>=2.5.0 (from pooch>=1.0->librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading platformdirs-4.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->librosa==0.10.1->-r requirements.txt (line 5))\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets==2.17.1->-r requirements.txt (line 1)) (2022.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting requests>=2.19.0 (from datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 1))\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 1)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==2.17.1->-r requirements.txt (line 1)) (2020.6.20)\n",
      "Collecting sympy (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 3)) (3.0.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton<3,>=2.0.0 (from openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.1->-r requirements.txt (line 1)) (1.16.0)\n",
      "Collecting mpmath>=0.19 (from sympy->torch->openai-whisper==20231117->-r requirements.txt (line 3))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.7/536.7 kB 43.1 MB/s eta 0:00:00\n",
      "Downloading python_swiftclient-4.4.0-py3-none-any.whl (88 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.9/88.9 kB 16.9 MB/s eta 0:00:00\n",
      "Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 19.0 MB/s eta 0:00:00\n",
      "Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 253.7/253.7 kB 31.2 MB/s eta 0:00:00\n",
      "Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 39.8 MB/s eta 0:00:00\n",
      "Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 76.5 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 58.6 MB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 61.7 MB/s eta 0:00:00\n",
      "Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 71.3 MB/s eta 0:00:00\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.9/443.9 kB 40.2 MB/s eta 0:00:00\n",
      "Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 19.4 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 25.1 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 61.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 47.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 kB 33.6 MB/s eta 0:00:00\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.1/385.1 kB 38.0 MB/s eta 0:00:00\n",
      "Downloading numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/3.7 MB 83.9 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 59.8 MB/s eta 0:00:00\n",
      "Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 42.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 46.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 56.2 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 69.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.6/38.6 MB 42.6 MB/s eta 0:00:00\n",
      "Downloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 81.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 13.4 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Downloading lxml-5.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 68.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 20.6 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 68.1 MB/s eta 0:00:00\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 64.7 MB/s eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 11.4 MB/s eta 0:00:00\n",
      "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 20.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 22.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 34.0 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 16.7 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 15.7 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 28.4 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.1/142.1 kB 23.6 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 30.9 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 MB 40.2 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 18.3 MB/s eta 0:00:00\n",
      "Downloading platformdirs-4.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 30.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.4/345.4 kB 36.4 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 33.9 MB/s eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 62.8 MB/s eta 0:00:00\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.6/117.6 kB 19.9 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 68.9 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 45.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 62.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=2b795c0d6249f7b7740ba2dc673e11f427553f531815a9122d55a8e55d163f15\n",
      "  Stored in directory: /home/cc/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: sentencepiece, mpmath, xxhash, tzdata, typing-extensions, tqdm, threadpoolctl, tabulate, sympy, safetensors, regex, python-dateutil, pycparser, pycountry, pyarrow-hotfix, portalocker, platformdirs, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgpack, lxml, llvmlite, joblib, fsspec, frozenlist, filelock, dill, decorator, charset-normalizer, audioread, async-timeout, yarl, triton, soxr, scipy, sacrebleu, requests, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nltk, multiprocess, lazy-loader, cffi, aiosignal, tiktoken, soundfile, scikit-learn, python-swiftclient, pooch, nvidia-cusolver-cu12, huggingface-hub, aiohttp, torch, tokenizers, librosa, transformers, openai-whisper, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 audioread-3.0.1 cffi-1.16.0 charset-normalizer-3.3.2 datasets-2.17.1 decorator-5.1.1 dill-0.3.8 filelock-3.13.4 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.22.2 joblib-1.4.0 lazy-loader-0.4 librosa-0.10.1 llvmlite-0.42.0 lxml-5.2.1 mpmath-1.3.0 msgpack-1.0.8 multidict-6.0.5 multiprocess-0.70.16 networkx-3.3 nltk-3.8.1 numba-0.59.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 packaging-24.0 pandas-2.2.2 platformdirs-4.2.0 pooch-1.8.1 portalocker-2.8.2 pyarrow-15.0.2 pyarrow-hotfix-0.6 pycountry-23.12.11 pycparser-2.22 python-dateutil-2.9.0.post0 python-swiftclient-4.4.0 regex-2024.4.16 requests-2.31.0 sacrebleu-2.4.0 safetensors-0.4.3 scikit-learn-1.4.2 scipy-1.13.0 sentencepiece-0.2.0 soundfile-0.12.1 soxr-0.3.7 sympy-1.12 tabulate-0.9.0 threadpoolctl-3.4.0 tiktoken-0.6.0 tokenizers-0.15.2 torch-2.2.2 tqdm-4.66.2 transformers-4.38.0 triton-2.2.0 typing-extensions-4.11.0 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='pip install -r requirements.txt' exited=0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = ssh.Remote(reserved_fip)\n",
    "node.run('wget https://raw.githubusercontent.com/shreyjasuja/re_s2st/main/requirements.txt -O requirements.txt')\n",
    "node.run('pip install -r requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download CoVoST 2 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoVoST 2 is a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. The dataset is created using Mozilla's open-source Common Voice database of crowdsourced voice recordings. There are 2,900 hours of speech represented in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is available as part of Hugging Face **datasets** library, but would require manual download of audio files for each source language from [Common Voices database](https://commonvoice.mozilla.org/en/datasets). CoVoST 2 consists of the following translation data as stated on its [github](https://github.com/facebookresearch/covost?tab=readme-ov-file#covost-2):\n",
    "\n",
    "**X into English:** French, German, Spanish, Catalan, Italian, Russian, Chinese, Portuguese, Persian, Estonian, Mongolian, Dutch, Turkish, Arabic, Swedish, Latvian, Slovenian, Tamil, Japanese, Indonesian, Welsh\n",
    "\n",
    "**English into X:** German, Catalan, Chinese, Persian, Estonian, Mongolian, Turkish, Arabic, Swedish, Latvian, Slovenian, Tamil, Japanese, Indonesian, Welsh\n",
    "\n",
    "So, we would need to download individual voice data from [Common Voices database](https://commonvoice.mozilla.org/en/datasets) for each source language defined above. Please follow the following steps:\n",
    "\n",
    "1. Download the shell-script onto the server to help downloading audio files in appropriate directory structure.\n",
    "Please also provide execute permission to this shell script using `chmod +x download_data.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you have already downloaded this data in the past and persisted this data onto the Object Store. You can directly go to [Step 2](#step2) in the \\[Optional\\] Object Store section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-14 22:23:20--  https://raw.githubusercontent.com/shreyjasuja/re_s2st/main/scripts/download_data.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1251 (1.2K) [text/plain]\n",
      "Saving to: ‘download_data.sh’\n",
      "\n",
      "     0K .                                                     100% 41.0M=0s\n",
      "\n",
      "2024-04-14 22:23:20 (41.0 MB/s) - ‘download_data.sh’ saved [1251/1251]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='chmod +x download_data.sh' exited=0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('wget https://raw.githubusercontent.com/shreyjasuja/re_s2st/main/scripts/download_data.sh -O download_data.sh')\n",
    "node.run('chmod +x download_data.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go to [Common Voices](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fcommonvoice.mozilla.org%2Fen%2Fdatasets) and select the source language\n",
    "\n",
    "3. Make sure to choose **Common Voice Corpus 4** as the version\n",
    "\n",
    "4. Fill in your email. Read and accept necessary T&Cs.\n",
    "\n",
    "5. Don't click and submit yet, instead copy the link from **Download Dataset Bundle** button\n",
    "\n",
    "6. use `download_data.sh` to download this data and execute the file while passing url as the argument \\\n",
    "`!./download_data.sh  \"<link_to_download>\"`\n",
    "\n",
    "**Note**:  `download_data.sh` would help in downloading the data in a specific directory structure based on language codes, kindly don't modify this shell-script so that the inference code runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example, this link was running at the time of creation of this notebook\n",
    "node = ssh.Remote(reserved_fip)\n",
    "node.run('./download_data.sh \"https://storage.googleapis.com/common-voice-prod-prod-datasets/cv-corpus-4-2019-12-10/nl.tar.gz?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gke-prod%40moz-fx-common-voice-prod.iam.gserviceaccount.com%2F20240403%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240403T000757Z&X-Goog-Expires=43200&X-Goog-SignedHeaders=host&X-Goog-Signature=22e9073d7737d426b14fc059d289e1e08b017fdd0b71b3079b6a9a639700b3d4a4db9df36a32c11d6591369e8dd0ff8442b6a4de00f5e04d776e1c7e0a199329f571cb9a6e46ed6735e76b3a3a4a21ee91060a36890b9806a6016e3a2b6245a9b7bb44ba5e1945b6b555bb1fd72e118fc12ecdfe8df7f2dd936b453a123879af1581691825677ceff1ab2e0e0396b225b793c4f1993463b432295e17f56747c53f8bd154b07cf67980a0af9660023f0abd0e4679fa1a8dcea4e8a2acbed0bdf111f774697c340ed1c910be552b92cb2c8fe87200fefabb787844266ca3595f060aebcd9e75438591301965d97fddd0d7434118ca7e7771bf9a3a02685774477a\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat the above steps again and again (21 times)** to download source audios for all languages. Once that is done you would notice that a folder named `data` is created with corresponding 'language code' as sub-directories and each sub-directory contains `tar.gz` (compressed) file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Optional: Persist the data on Object Store*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is completely optional and it persists downloaded data on ObjectStore. As we saw that the download links for source audio bear authentication which is set to expire, we would want to save ourselves from redundant effort of downloading the data, by persisting it in Chameleon Object Store containers. These containers can be accessed on any baremetal instance on Chameleon. Chameleon offers this functionality using containers within [Object Store](https://chameleoncloud.readthedocs.io/en/latest/technical/swift.html#id2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would involve two steps:\n",
    "\n",
    "1. Intialize the container and upload the data.\n",
    "2. Download the data for subsequent experiment, directly from the object store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would be performing just step 1 here to upload data and would perform step 2 on the server whenever the access to this data is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need additional packages, you can install them in a similar manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you need to persist the data, make sure you upload the data, before actually extracting the `tar.gz` files. This will not only help you save the space on object store but also lead to faster downloads and uploads.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the containers already on the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'CoVoST2_data', 'count': 43, 'bytes': 12620282177}\n",
      "{'name': 'CoVoST2_data_segments', 'count': 13, 'bytes': 48142691030}\n",
      "{'name': 'my-new-container', 'count': 43, 'bytes': 12620279870}\n",
      "{'name': 'my-new-container_segments', 'count': 13, 'bytes': 48142691030}\n"
     ]
    }
   ],
   "source": [
    "from chi.clients import session\n",
    "from swiftclient.client import Connection\n",
    "\n",
    "swift_conn = Connection(session=session())\n",
    "resp_headers, containers = swift_conn.get_account()\n",
    "\n",
    "for container in containers:\n",
    "    print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a container initialized in Object Store. You can use that to upload the files or else you can initilaize a new container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Step 1: Intialize the container and upload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to initialize a new container\n",
    "container_name = 'my-new-container'\n",
    "swift_conn.put_container(container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a container ready. Lets transfer the downloaded files to the container. The upload would take around 8 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = ssh.Remote(reserved_fip)\n",
    "node.run('source openrc && swift --os-auth-type v3applicationcredential upload --changed --segment-size 4831838208 '+container_name+' data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Step 2: Download the persisted CoVoST2 data directly from the object store. <a id=\"step2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would download the `data` directory object from your container. This would take approximately 4-5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name='CoVoST2_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result cmd='source openrc && openstack container save CoVoST2_data' exited=0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('source openrc && openstack container save '+ container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Jupyter on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `jupyter_http_over_ws`, which is required in order to connect Colab to this Jupyter instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run('pip install jupyter-core==5.5.0 jupyter-client==6.1.12 jupyter-server==1.24.0 jupyterlab-widgets==3.0.9  jupyterlab==3.6.6 jupyter_http_over_ws traitlets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, activate `jupyter_http_over_ws` - the output should show `jupyter_http_over_ws 0.0.7 OK` (don't worry if it also says \"Validation failed\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling: jupyter_http_over_ws\n",
      "- Writing config: /usr/etc/jupyter\n",
      "    - Validating jupyter_http_over_ws...\n",
      "      jupyter_http_over_ws 0.0.7 \u001b[32mOK\u001b[0m\n",
      "     \u001b[31m X\u001b[0m Validation failed: [Errno 13] Permission denied: '/usr/etc'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Result cmd='jupyter server extension enable --py jupyter_http_over_ws' exited=0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.run('jupyter server extension enable --py jupyter_http_over_ws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Colab to the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **local terminal on your own laptop**, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -L 127.0.0.1:8889:127.0.0.1:8889 cc@192.5.86.150\n"
     ]
    }
   ],
   "source": [
    "print('ssh -L 127.0.0.1:8889:127.0.0.1:8889 cc@' + reserved_fip) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to set up a tunnel to the Jupyter server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If your Chameleon key is not in the default location, you should also specify the path to your key as an argument, using `-i`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i ~/.ssh/id_rsa_chameleon -L 127.0.0.1:8889:127.0.0.1:8889 cc@192.5.86.150\n"
     ]
    }
   ],
   "source": [
    "print('ssh -i ~/.ssh/id_rsa_chameleon -L 127.0.0.1:8889:127.0.0.1:8889 cc@' + reserved_fip) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave this SSH session open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following cell, which will run a command that does not terminate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 01:35:07.257 NotebookApp] Writing notebook server cookie secret to /home/cc/.local/share/jupyter/runtime/notebook_cookie_secret\n",
      "[W 01:35:07.561 NotebookApp] Loading JupyterLab as a classic notebook (v6) extension.\n",
      "[W 2024-04-20 01:35:07.562 LabApp] 'allow_origin' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "[W 2024-04-20 01:35:07.562 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "[W 2024-04-20 01:35:07.562 LabApp] 'port_retries' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "[W 2024-04-20 01:35:07.562 LabApp] 'port_retries' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "[I 2024-04-20 01:35:07.566 LabApp] JupyterLab extension loaded from /home/cc/.local/lib/python3.10/site-packages/jupyterlab\n",
      "[I 2024-04-20 01:35:07.566 LabApp] JupyterLab application directory is /home/cc/.local/share/jupyter/lab\n",
      "[I 01:35:07.569 NotebookApp] Serving notebooks from local directory: /home/cc\n",
      "[I 01:35:07.569 NotebookApp] Jupyter Notebook 6.5.6 is running at:\n",
      "[I 01:35:07.569 NotebookApp] http://localhost:8889/?token=4de974fa18e14f54d64eedf102a94a8da33474cfc5d04194\n",
      "[I 01:35:07.569 NotebookApp]  or http://127.0.0.1:8889/?token=4de974fa18e14f54d64eedf102a94a8da33474cfc5d04194\n",
      "[I 01:35:07.569 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[W 01:35:07.571 NotebookApp] No web browser found: could not locate runnable browser.\n",
      "[C 01:35:07.571 NotebookApp] \n",
      "    \n",
      "    To access the notebook, open this file in a browser:\n",
      "        file:///home/cc/.local/share/jupyter/runtime/nbserver-2928-open.html\n",
      "    Or copy and paste one of these URLs:\n",
      "        http://localhost:8889/?token=4de974fa18e14f54d64eedf102a94a8da33474cfc5d04194\n",
      "     or http://127.0.0.1:8889/?token=4de974fa18e14f54d64eedf102a94a8da33474cfc5d04194\n",
      "[W 01:37:24.002 NotebookApp] 404 OPTIONS /api/colab/build-info?token=4de974fa18e14f54d64eedf102a94a8da33474cfc5d04194 (127.0.0.1) 34.110000ms referer=https://colab.research.google.com/\n",
      "[W 01:37:24.033 NotebookApp] 404 GET /api/colab/build-info?token=4de974fa18e14f54d64eedf102a94a8da33474cfc5d04194 (127.0.0.1) 3.190000ms referer=https://colab.research.google.com/\n",
      "[I 01:37:24.174 NotebookApp] Kernel started: 1788cbfb-67d3-4742-8601-dad4900c666f, name: python3\n"
     ]
    }
   ],
   "source": [
    "node.run(\"jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8889 --NotebookApp.port_retries=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output of the cell above, look for a URL in this format:\n",
    "    \n",
    "```\n",
    "http://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this URL - you will need it in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open Colab in a browser. Click on the drop-down menu for \"Connect\" in the top right and select \"Connect to a local runtime\". Paste the URL you copied earlier into the space and click \"Connect\". Your notebook should now be running on your Colab host (you can put `!hostname` in a cell and run it to verify!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release resources\n",
    "\n",
    "If you finish with your experimentation before your lease expires,release your resources and tear down your environment by running the following (commented out to prevent accidental deletions).\n",
    "\n",
    "This section is designed to work as a \"standalone\" portion - you can come back to this notebook, ignore the top part, and just run this section to delete your reasources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using CHI@UC:\n",
      "URL: https://chi.uc.chameleoncloud.org\n",
      "Location: Argonne National Laboratory, Lemont, Illinois, USA\n",
      "Support contact: help@chameleoncloud.org\n"
     ]
    }
   ],
   "source": [
    "# setup environment - if you made any changes in the top part, make the same changes here\n",
    "import chi, os\n",
    "from chi import lease, server\n",
    "\n",
    "PROJECT_NAME = os.getenv('OS_PROJECT_NAME')\n",
    "chi.use_site(\"CHI@UC\")\n",
    "chi.set(\"project_name\", PROJECT_NAME)\n",
    "username = os.getenv('USER') # all exp resources will have this prefix\n",
    "NODE_TYPE=\"gpu_rtx_6000\"\n",
    "lease = chi.lease.get_lease(f\"colab-{username}-{NODE_TYPE}-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No matching servers found for name \"colab-sj4020_nyu_edu-gpu_rtx_6000-v2\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103/2193262130.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# get info about resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mserver_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_server_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"colab-{username}-{NODE_TYPE}-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mreserved_fip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'addr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddresses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sharednet1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OS-EXT-IPS:type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'floating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/chi/server.py\u001b[0m in \u001b[0;36mget_server_id\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mservers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnova\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mservers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No matching servers found for name \"{name}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Multiple matching servers found for name \"{name}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No matching servers found for name \"colab-sj4020_nyu_edu-gpu_rtx_6000-v2\""
     ]
    }
   ],
   "source": [
    "DELETE = False\n",
    "# DELETE = True  # un-comment if you're ready to delete resources\n",
    "\n",
    "if DELETE:\n",
    "\n",
    "    # get info about resources\n",
    "    server_id = chi.server.get_server_id(f\"colab-{username}-{NODE_TYPE}-v1\")\n",
    "    reserved_fip = [d['addr'] for d in chi.server.show_server(server_id).addresses['sharednet1'] if d['OS-EXT-IPS:type']=='floating'][0]\n",
    "\n",
    "    # delete server   \n",
    "    chi.server.delete_server(server_id)\n",
    "\n",
    "    # # release floating IP\n",
    "    reserved_fip =  chi.lease.get_reserved_floating_ips(lease[\"id\"])\n",
    "    ip_info = chi.network.get_floating_ip(reserved_fip)\n",
    "    chi.neutron().delete_floatingip(ip_info[\"id\"])\n",
    "\n",
    "    # delete lease\n",
    "    chi.lease.delete_lease(lease[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
