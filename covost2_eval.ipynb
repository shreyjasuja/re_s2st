{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5U3YBWdWOs4"
      },
      "source": [
        "# Evaluation on CoVoST2 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij8C5NCMy_O_"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/shreyjasuja/re_s2st/blob/main/covost2_eval.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook reproduces evaluation results of three models on CoVoST2 dataset:\n",
        "\n",
        "*   [Whisper](https://arxiv.org/pdf/2212.04356.pdf) (Radford et al.,2022)\n",
        "*   [SeamlessM4T](https://arxiv.org/pdf/2308.11596.pdf) (Barrault et al.,2023)\n",
        "*   [XLS-R](https://arxiv.org/pdf/2111.09296.pdf) (Babu et al.,2021)\n",
        "\n",
        "\n",
        "CoVoST 2 is a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. The dataset is created using Mozilla's open-source Common Voice database of crowdsourced voice recordings. There are 2,900 hours of speech represented in the corpus.\n",
        "\n",
        "Although most of these models are multi-task models, we would be focusing here on their multilingual translation capabilities\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HYr_7xCnx0PH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data from container to the disk"
      ],
      "metadata": {
        "id": "bPSkDm61XQtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provided that we downloaded the data from Common Voices dataset and persisted on Chameleon container, we can now load this data from the container to our disk. \\\\\n",
        "Lets first see what containers we do have on our account"
      ],
      "metadata": {
        "id": "t_2-E1SOwI7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-BYPyC-bY9t",
        "outputId": "3458227b-9bf9-4fd4-91a2-ea1c9a4105c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your password: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import subprocess\n",
        "\n",
        "command = ['bash', '-c', 'source CHI-231138-openrc.sh && openstack container list']\n",
        "password = getpass(\"Please enter your password: \")  # Use getpass.getpass() to input this securely as shown above\n",
        "\n",
        "proc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "stdout, stderr = proc.communicate(input=password)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84JFm5QhcIu2",
        "outputId": "eeb3e180-70dd-4886-b8d2-ef5d785f8a16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(sj4020@nyu.edu) Please enter your Chameleon CLI password: \n",
            "+-----------------------+\n",
            "| Name                  |\n",
            "+-----------------------+\n",
            "| CoVoST2_data          |\n",
            "| CoVoST2_data_segments |\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the container where your data persists"
      ],
      "metadata": {
        "id": "v3_iWZ2axtT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "container_name=\"CoVoST2_data\""
      ],
      "metadata": {
        "id": "CBZJFePexm84"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now download all the data from container. This should take around 5 minutes to execute."
      ],
      "metadata": {
        "id": "zcOuVkh-X3TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "command = ['bash', '-c', 'source CHI-231138-openrc.sh && openstack container save '+container_name]\n",
        "password = getpass(\"Please enter your password: \")  # Use getpass.getpass() to input this securely as shown above\n",
        "\n",
        "proc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "stdout, stderr = proc.communicate(input=password)"
      ],
      "metadata": {
        "id": "Ogd3uH2oXbFy",
        "outputId": "6fcfe8ae-6e56-4c89-a932-a57f90fddf99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your password: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember from the notebook where we downloaded the audio data, we saved compressed files and a script to extract these later. We would now use that script to extract all our audio files in the required directory structure. This will take approx 8-10 minutes."
      ],
      "metadata": {
        "id": "rDSIWMwl7IZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WFTnq59Ke6le",
        "outputId": "88f768ff-a758-4d98-dbf6-3f3a7aaace91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.56 s, sys: 1.55 s, total: 11.1 s\n",
            "Wall time: 7min 38s\n"
          ]
        }
      ],
      "source": [
        "%time !(cd data && chmod +x extract_and_cleanup.sh && ./extract_and_cleanup.sh) &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have only loaded audio files till now. We would also require the trancriptions and/or translations as ground truth for our evaluation. This reference textual data is provided by Hugging face ü§ó Datasets library [here](https://huggingface.co/datasets/covost2)."
      ],
      "metadata": {
        "id": "lbaFdyNi1ksY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try loading some language, say Catalan and see how the data looks like. Language code for Catalan is `ca`."
      ],
      "metadata": {
        "id": "CFzFfYNV2na2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "0mxabTh83EUV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=load_dataset(\"covost2\",\"ca_en\",data_dir=\"data/ca\",split=\"test\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "IN1hgmDV275r"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each data point will have the audio file `path` to the audio we downloaded before, an audio `array` which is already sampled at sampling rate of 16,000, transcription in source language as `sentence` and translation to english as `translation` field."
      ],
      "metadata": {
        "id": "eCNFnlR3_gkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "id": "mtMt4kUQ3Zv5",
        "outputId": "59d5cfd7-432d-42f0-ccc0-14420fa55f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'client_id': '03de40b6ecf87f9e1f42719a857b2fbf3b93179bf443e707870f2dda3e53b621248065d52be4dfa6ec462fe118b76b345c19e14063b840813a369c54aab6e1c6',\n",
              " 'file': '/home/cc/data/ca/clips/common_voice_ca_19034690.mp3',\n",
              " 'audio': {'path': '/home/cc/data/ca/clips/common_voice_ca_19034690.mp3',\n",
              "  'array': array([ 2.32830644e-10, -1.74622983e-10, -3.25962901e-09, ...,\n",
              "          9.91155393e-04, -7.40018208e-04, -5.23986295e-04]),\n",
              "  'sampling_rate': 16000},\n",
              " 'sentence': '\"Supervisa l\\'emissi√≥ de les resolucions de concessi√≥ de l\\'habitaci√≥.\"',\n",
              " 'translation': 'Supervises issuance of room concession decisions.',\n",
              " 'id': 'common_voice_ca_19034690'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speeches=[]\n",
        "transcripts=[]\n",
        "\n",
        "for i in data:\n",
        "  speeches.append(i['audio']['path'])\n",
        "  transcripts.append(i['sentence'])"
      ],
      "metadata": {
        "id": "aGHd2NrH-mY2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxQGPannUOqH"
      },
      "source": [
        "#### Browse data in VizSeq (see also the [VizSeq documentation](https://facebookresearch.github.io/vizseq/))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vizseq\n"
      ],
      "metadata": {
        "id": "I60Vrwc33uUr",
        "outputId": "2a592e5c-0c84-4adf-ad7d-6474173474da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: vizseq\n",
            "Successfully installed vizseq-0.1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydeqyUaul46J"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu==2.3.1"
      ],
      "metadata": {
        "id": "ATCzVk5xAUYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divide language in different categories"
      ],
      "metadata": {
        "id": "PWfQG6Dw1VAd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4CZ7zYNUAEB"
      },
      "source": [
        " While evaluating performance in terms of translation capabilities, we need to divide our languages between high, mid and low resource categories depending on what amount of data is available in each language. This distribution has been provided by Babu et al.,2021 in their XLS-R [paper](https://arxiv.org/pdf/2111.09296.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_levels=[\"low_res\",\"mid_res\",\"high_res\"]"
      ],
      "metadata": {
        "id": "OJlsyoIB6CM8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_res=['ca','de','fr','es']\n",
        "mid_res=['zh-CN','fa','it','ru','pt']\n",
        "low_res=['mn','ta','lv','et','cy','sl','ja','tr','ar','nl','sv-SE','id']"
      ],
      "metadata": {
        "id": "OZDg567i6Eb7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics"
      ],
      "metadata": {
        "id": "6rMztvccFugx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use BLEU score as our evaluation metric. We will source this implementation from the sacrebleu library which is consistent with methodology cited in the research papers. SeamlessM4T also presented the score using same library implementation for *sacrebleu version 2.3.1*"
      ],
      "metadata": {
        "id": "eoCO0yHqGLQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_sacre_bleu(translations,gt_translations):\n",
        "  #calculate BLEU score\n",
        "  bleu = sacrebleu.corpus_bleu(translations, [gt_translations])\n",
        "  return round(bleu.score, 3)"
      ],
      "metadata": {
        "id": "wCdJDgbGF8IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or else we could have also used NLTK's BLEU score implementation, for which scoring function would have look like this."
      ],
      "metadata": {
        "id": "cPR5HNXGH5RY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPx8JkeGml-4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "def evaluate_nltk_bleu(translations,gt_translations):\n",
        "  references = [[word_tokenize(ref)] for ref in gt_translations]\n",
        "  candidates = [word_tokenize(cand) for cand in translations]\n",
        "  bleu_score=corpus_bleu(list_of_references=references,hypotheses=candidates)\n",
        "  return round(bleu_score * 100, 3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AY5FCQD1RuU"
      },
      "source": [
        "## Load Whisper and do inference on CoVoST2 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple whisper mode with varying size. Out of these `large-v2` being the largest of all, tends to perform best. So, we reproduce the results for Whisper large-v2 model only for comparative analysis."
      ],
      "metadata": {
        "id": "g6TpWVQA6RRw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "wNa6oB8WofoD",
        "outputId": "5d993f68-241a-4ed4-c65c-11269b394253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.87G/2.87G [01:20<00:00, 38.2MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"large-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "id": "NUqVTRUNL7G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWcM0iMlplvZ",
        "outputId": "f40f2038-3154-4b55-e6d1-cd02d3a636b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is multilingual and has 1,541,384,960 parameters.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6zRjGcQUx5Q"
      },
      "source": [
        "Below is the function which runs a source langauge to infer over X-eng translations.\n",
        "\n",
        "The parameters defined under `options` is consistent with the example [notebook](https://github.com/openai/whisper/blob/main/notebooks/Multilingual_ASR.ipynb) shared by Whisper for multilingual translation on its github implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "sSaBRfdy1mfy"
      },
      "outputs": [],
      "source": [
        "def whisper_inference(src_lang):\n",
        "  x_en=load_dataset(\"covost2\",src_lang+\"_en\",data_dir=\"data/\"+src_lang,split=\"test\",trust_remote_code=True)\n",
        "\n",
        "  options = dict(language=src_lang.split('-')[0], beam_size=5, best_of=5)\n",
        "  # transcribe_options = dict(task=\"transcribe\",**options))\n",
        "  translate_options = dict(task=\"translate\",**options)\n",
        "\n",
        "  translations = []\n",
        "  gt_translations = []\n",
        "\n",
        "  # transcriptions = []\n",
        "  # gt_transcripts=[]\n",
        "\n",
        "\n",
        "  for item in tqdm(x_en):\n",
        "      audio = item['file']\n",
        "\n",
        "      translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
        "      translations.append(translation)\n",
        "      gt_translations.append(item['translation'])\n",
        "\n",
        "      # transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "      # transcriptions.append(transcription)\n",
        "      # gt_transcripts.append(item['sentence'])\n",
        "  return translations, gt_translations\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "EjE17ileSj_w"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "whisper_bleu_score=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tGSG328iA2R"
      },
      "outputs": [],
      "source": [
        "file_name = 'whisper_largev2_covost2_score.json'\n",
        "for i in res_levels[:1]:\n",
        "  for src in eval(i)[:1]:\n",
        "    translations, ground_truth=whisper_inference(src)\n",
        "    whisper_bleu_score[i][src]=evaluate_sacre_bleu(translations=translations,gt_translations=ground_truth)\n",
        "  with open(file_name, 'w') as f:\n",
        "    json.dump(whisper_bleu_score, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clear GPU memory\n",
        "import torch\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YY2R064WJSH5"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYrhKqeBeU_G"
      },
      "source": [
        "## Load XLS-R (2B) model and do infernece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the huggingface ü§ó transformers implementation of XLS-R (2B) model.\n",
        "\n",
        "We would be using `wav2vec2-xls-r-2b-21-to-en` model as it is a encoder-decoder model which has been fine-tuned to support languages in CoVoST2 X-eng translations. The details about which can be found [here](https://huggingface.co/facebook/wav2vec2-xls-r-2b-21-to-en)"
      ],
      "metadata": {
        "id": "5hKq98-nJ0As"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Please beaware that the reference code given for inference at huggingface doesn't work, please use the below implementation"
      ],
      "metadata": {
        "id": "2i9Ndk90LJ9J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "DAboaE2kgyyG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import SpeechEncoderDecoderModel,MBart50Tokenizer\n",
        "from datasets import load_dataset\n",
        "#loading the MBart50Tokenizer as decoder is MBart50 transformer model\n",
        "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "i09XNcgD-3h4"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(\"facebook/wav2vec2-xls-r-2b-21-to-en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "AwwCHQ6UZnRf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pipleine function to put together the tokenizer, feature extractor and the actual model"
      ],
      "metadata": {
        "id": "vvpiR5YYOOgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnWbf-JEICo2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "asr=pipeline(model=\"facebook/wav2vec2-xls-r-2b-21-to-en\",tokenizer=tokenizer,feature_extractor=feature_extractor,device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "RRwFVWA1pUte"
      },
      "outputs": [],
      "source": [
        "def xlsr_inference(src_lang):\n",
        "  x_en=load_dataset(\"covost2\",src_lang+\"_en\",data_dir=\"data/\"+src_lang,split=\"test\",trust_remote_code=True)\n",
        "\n",
        "  translations = []\n",
        "  gt_translations = []\n",
        "\n",
        "  # transcriptions = []\n",
        "  # gt_transcripts=[]\n",
        "\n",
        "\n",
        "  for item in tqdm(x_en):\n",
        "      audio = item['file']\n",
        "\n",
        "      translation = asr(audio)[\"text\"]\n",
        "      translations.append(translation)\n",
        "      gt_translations.append(item['translation'])\n",
        "\n",
        "      # transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "      # transcriptions.append(transcription)\n",
        "      # gt_transcripts.append(item['sentence'])\n",
        "  return translations, gt_translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "TiZPjlBJXDD1"
      },
      "outputs": [],
      "source": [
        "xlsr_bleu_score=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty8crgbd_bfM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "file_name = 'xls_r_covost2_score.json'\n",
        "for i in res_levels:\n",
        "  for src in eval(i):\n",
        "    translations, ground_truth=xlsr_inference(src)\n",
        "    xlsr_bleu_score[i][src]=evaluate_sacre_bleu(translations=translations,gt_translations=ground_truth)\n",
        "    with open(file_name, 'w') as f:\n",
        "      json.dump(xlsr_bleu_score, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpyW1Gmx_jRV",
        "outputId": "37850e5c-2347-4b59-8c56-ae0cdcf2c5f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(dict,\n",
              "            {'low_res': {'mn': 1.877,\n",
              "              'ta': 0.613,\n",
              "              'lv': 20.774,\n",
              "              'et': 11.186,\n",
              "              'cy': 14.671,\n",
              "              'sl': 19.117,\n",
              "              'ja': 4.102,\n",
              "              'tr': 16.774,\n",
              "              'ar': 16.991,\n",
              "              'nl': 31.883,\n",
              "              'sv-SE': 30.987,\n",
              "              'id': 16.255},\n",
              "             'mid_res': {'zh-CN': 9.475,\n",
              "              'fa': 13.073,\n",
              "              'it': 35.034,\n",
              "              'ru': 39.44,\n",
              "              'pt': 42.012},\n",
              "             'high_res': {'ca': 33.813,\n",
              "              'de': 33.486,\n",
              "              'fr': 37.614,\n",
              "              'es': 39.166}})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xlsr_bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR5TdYiQnS6P"
      },
      "source": [
        "# Seamless medium on CoVoST 2 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1YkQLMTuF8Q",
        "outputId": "c1ffbbaf-a527-47f4-d6cf-b274ebba11b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/cc/.local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNt_E2Nwu29J"
      },
      "outputs": [],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01h2MNzOnibE"
      },
      "outputs": [],
      "source": [
        "def seamless_inference(src_lang):\n",
        "  x_en=load_dataset(\"covost2\",src_lang+\"_en\",data_dir=\"data/\"+src_lang,split=\"test\",trust_remote_code=True)\n",
        "\n",
        "  translations = []\n",
        "  gt_translations = []\n",
        "\n",
        "  # transcriptions = []\n",
        "  # gt_transcripts=[]\n",
        "\n",
        "\n",
        "  for item in tqdm(x_en):\n",
        "      audio_sample = item['audio']\n",
        "      audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\",sampling_rate=16000)\n",
        "      audio_inputs = {k: v.to('cuda') for k, v in audio_inputs.items()}\n",
        "\n",
        "      output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\",generate_speech=False)\n",
        "      translation=processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "      translations.append(translation)\n",
        "      gt_translations.append(item['translation'])\n",
        "\n",
        "      # transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "      # transcriptions.append(transcription)\n",
        "      # gt_transcripts.append(item['sentence'])\n",
        "  return translations, gt_translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDEWrOs1pzYE"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "import collections\n",
        "seamless_bleu_score=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLJm88V5t1DY",
        "outputId": "7ce40f6a-d72a-4bf1-e4d6-042c1666e56b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1759/1759 [12:11<00:00,  2.41it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 786/786 [03:29<00:00,  3.75it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1629/1629 [06:33<00:00,  4.14it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1571/1571 [13:28<00:00,  1.94it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 690/690 [03:42<00:00,  3.10it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360/360 [01:46<00:00,  3.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 684/684 [03:21<00:00,  3.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1629/1629 [08:26<00:00,  3.21it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1695/1695 [07:31<00:00,  3.76it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1699/1699 [08:25<00:00,  3.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1595/1595 [06:42<00:00,  3.97it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 844/844 [03:24<00:00,  4.12it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4898/4898 [32:36<00:00,  2.50it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3445/3445 [17:31<00:00,  3.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8951/8951 [56:18<00:00,  2.65it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6300/6300 [41:09<00:00,  2.55it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4023/4023 [20:34<00:00,  3.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12730/12730 [1:18:42<00:00,  2.70it/s]\n",
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6766/13511 [40:45<37:00,  3.04it/s]"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "file_name = 'seamless_large_sacre_bleu.json'\n",
        "for i in res_levels:\n",
        "  for src in eval(i):\n",
        "    translations, ground_truth=seamless_inference(src)\n",
        "    seamless_bleu_score[i][src]=evaluate_sacre_bleu(translations=translations,gt_translations=ground_truth)\n",
        "    with open(file_name, 'w') as f:\n",
        "      json.dump(seamless_bleu_score, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTlATNFoUvm7",
        "outputId": "3db0ba9f-08ad-444c-a006-b58a4313958f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(dict,\n",
              "            {'low_res': {'mn': 7.378,\n",
              "              'ta': 3.931,\n",
              "              'lv': 26.902,\n",
              "              'et': 26.298,\n",
              "              'cy': 55.276,\n",
              "              'sl': 38.32,\n",
              "              'ja': 19.69,\n",
              "              'tr': 30.647,\n",
              "              'ar': 45.732,\n",
              "              'nl': 40.112,\n",
              "              'sv-SE': 37.972,\n",
              "              'id': 50.455},\n",
              "             'mid_res': {'zh-CN': 19.911,\n",
              "              'fa': 25.213,\n",
              "              'it': 38.805,\n",
              "              'ru': 47.881,\n",
              "              'pt': 49.055},\n",
              "             'high_res': {'ca': 37.969,\n",
              "              'de': 38.009,\n",
              "              'fr': 40.724,\n",
              "              'es': 40.639}})"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seamless_bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlPgV4XuV41"
      },
      "outputs": [],
      "source": [
        "s=0\n",
        "for i in res_levels:\n",
        "  for k in eval(i):\n",
        "    s+=seamless_bleu_score[i][k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s8cjKrsvTJ6",
        "outputId": "f8e7af91-b0e7-4d27-a8ee-43e22a551336"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31.29719047619048"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s/21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3q56_7EvbsR",
        "outputId": "ce2492ba-ce99-4217-a895-1ff281bd9873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34.32947619047619"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s/21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcjLYl0FU1pO"
      },
      "source": [
        "## FLEURS DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzSLzRZ3pZpg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G66B1hRoU1QI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pycountry import languages\n",
        "def bcp47_to_iso639_3(bcp47_code):\n",
        "    parts = bcp47_code.split('_')\n",
        "    lang_code = parts[0]\n",
        "    try:\n",
        "\n",
        "      lang = languages.get(alpha_2=lang_code).alpha_3\n",
        "      return lang.lower()\n",
        "    except (AttributeError, KeyError) as e:\n",
        "        # If the mapping fails, return the original code\n",
        "        print(e, lang_code)\n",
        "        return lang_code.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHIYGM2yjNOr"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_config_names\n",
        "bcp_47_codes=get_dataset_config_names(\"google/fleurs\",trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LFdNikSU9UL",
        "outputId": "34e9b3c7-01f6-4c30-bf1a-ef9f8df31190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'alpha_3' ast\n",
            "'NoneType' object has no attribute 'alpha_3' ceb\n",
            "'NoneType' object has no attribute 'alpha_3' ckb\n",
            "'NoneType' object has no attribute 'alpha_3' cmn\n",
            "'NoneType' object has no attribute 'alpha_3' fil\n",
            "'NoneType' object has no attribute 'alpha_3' kam\n",
            "'NoneType' object has no attribute 'alpha_3' kea\n",
            "'NoneType' object has no attribute 'alpha_3' luo\n",
            "'NoneType' object has no attribute 'alpha_3' nso\n",
            "'NoneType' object has no attribute 'alpha_3' umb\n",
            "'NoneType' object has no attribute 'alpha_3' yue\n",
            "'NoneType' object has no attribute 'alpha_3' all\n"
          ]
        }
      ],
      "source": [
        "lang_dict={}\n",
        "for i in bcp_47_codes:\n",
        "  lang_dict[bcp47_to_iso639_3(i)]=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXNZiPrUbZYi",
        "outputId": "2ccdd99c-5a54-4dca-ca41-32d704c18973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-28 06:01:33--  https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.19, 3.162.163.11, 3.162.163.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6096377 (5.8M) [application/zip]\n",
            "Saving to: ‚Äòevaluation_data_ids.zip‚Äô\n",
            "\n",
            "evaluation_data_ids 100%[===================>]   5.81M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-03-28 06:01:33 (79.2 MB/s) - ‚Äòevaluation_data_ids.zip‚Äô saved [6096377/6096377]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAe8UrAHneNd"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install unzip\n",
        "!unzip evaluation_data_ids.zip && rm evaluation_data_ids.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2I1pSjmqi7_"
      },
      "outputs": [],
      "source": [
        "old_codes=['msa','fil','uzb','fas','nep','lav','ara','aze','pus','ori','mon','swa','orm']\n",
        "new_codes=['zlm','tgl','uzn','pes','npi','lvs','arb','azj','pbt','ory','khk','swh','gaz']\n",
        "for i in range(len(new_codes)):\n",
        "  lang_dict[new_codes[i]]=lang_dict[old_codes[i]]\n",
        "  del lang_dict[old_codes[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686SAdR3shmC",
        "outputId": "261377f8-f579-4a67-eac1-789c6b93ebb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_path=\"evaluation_data_ids/s2tt_fleurs_ids/\"\n",
        "x_eng_files = [file for file in os.listdir(\"evaluation_data_ids/s2tt_fleurs_ids/\") if file.endswith('-eng.ids')]\n",
        "print(len(x_eng_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFOtLKC1nhfw"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "generated_translations=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZYl81tzoHjK",
        "outputId": "36e04359-92a4-4dd8-830e-eba0cd8b4c4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict['ukr'],split=\"test\",streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XtWrSCg1Cj0",
        "outputId": "747e716c-8981-4fe7-f309-80742168be92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1982, 'num_samples': 118080, 'path': None, 'audio': {'path': 'test/10021730821550109934.wav', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.80006027e-05, -2.40206718e-05, -9.59634781e-05]), 'sampling_rate': 16000}, 'transcription': '–∂—ñ–Ω–∫–∏ —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏ —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É', 'raw_transcription': '–ñ—ñ–Ω–∫–∏: —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏, —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ, –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É.', 'gender': 0, 'lang_id': 92, 'language': 'Ukrainian', 'lang_group_id': 1}\n"
          ]
        }
      ],
      "source": [
        "print(next(iter(src_lang_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X37Ho9kPT9zV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWWb2q8vPCms"
      },
      "outputs": [],
      "source": [
        "lang_issue=[]\n",
        "lang_missing_ids=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p6PX1kn0xBa",
        "outputId": "020e7e09-9ee3-4938-ee6d-c2cda0d4b139"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [09:11<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 908/908 [11:45<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 926/926 [12:11<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 728/728 [08:41<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 749/749 [08:40<00:00,  1.44it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1015/1015 [12:48<00:00,  1.32it/s]\n",
            "264it [03:08,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1041/1041 [15:38<00:00,  1.11it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 918/918 [11:43<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 357/357 [04:26<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1021/1021 [13:00<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [11:24<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946/946 [11:01<00:00,  1.43it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 977/977 [11:28<00:00,  1.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 687/687 [08:17<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 857/857 [10:27<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 660/660 [11:18<00:00,  1.03s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 621/621 [09:49<00:00,  1.05it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792/792 [09:20<00:00,  1.41it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 925/925 [15:11<00:00,  1.01it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1008/1008 [17:31<00:00,  1.04s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 893/893 [11:12<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:18<00:00,  1.35it/s]\n",
            " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 237/379 [03:57<02:31,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 3189556219205510204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 312/379 [05:56<00:58,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 6634898757415929965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 379/379 [08:08<00:00,  1.29s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 905/905 [11:21<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 964/964 [13:15<00:00,  1.21it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 919/919 [11:39<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 920/920 [11:09<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 831/831 [10:39<00:00,  1.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 771/771 [10:06<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 364/364 [04:10<00:00,  1.45it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 979/979 [12:03<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 854/854 [12:44<00:00,  1.12it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 980/980 [12:53<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1019/1019 [13:19<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [05:31<00:00,  1.12it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 723/723 [09:26<00:00,  1.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1021/1021 [13:15<00:00,  1.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 862/862 [10:54<00:00,  1.32it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [03:40<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 871/871 [11:20<00:00,  1.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [09:15<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [11:28<00:00,  1.45it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 838/838 [10:20<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 969/969 [12:56<00:00,  1.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 574/574 [06:31<00:00,  1.47it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 726/726 [08:38<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 883/883 [10:43<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792/792 [09:56<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 856/856 [11:18<00:00,  1.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 862/862 [11:09<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 958/958 [11:56<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 851/851 [10:53<00:00,  1.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 923/923 [12:15<00:00,  1.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 922/922 [10:48<00:00,  1.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 428/428 [05:02<00:00,  1.41it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 834/834 [10:17<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [07:56<00:00,  1.26it/s]\n",
            " 14%|‚ñà‚ñé        | 55/405 [00:42<04:34,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 12560373056138365189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 405/405 [07:16<00:00,  1.08s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 984/984 [11:58<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 761/761 [10:01<00:00,  1.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 759/759 [09:22<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [06:43<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 998/998 [14:30<00:00,  1.15it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [08:00<00:00,  1.00s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 865/865 [11:54<00:00,  1.21it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 973/973 [12:13<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 382/382 [04:34<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 723/723 [08:57<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 932/932 [11:28<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 927/927 [11:11<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [08:19<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 883/883 [10:34<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 827/827 [13:04<00:00,  1.05it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 949/949 [11:16<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:39<00:00,  1.16it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541/541 [07:16<00:00,  1.24it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [11:13<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 775/775 [09:55<00:00,  1.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 819/819 [10:04<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [04:19<00:00,  1.01s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 487/487 [06:12<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 758/758 [08:53<00:00,  1.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 940/940 [11:49<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 986/986 [12:15<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 930/930 [11:23<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:36<00:00,  1.11it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 934/934 [11:19<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 418/418 [04:59<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 472/472 [05:40<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 700/700 [08:36<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 864/864 [10:51<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 967/967 [13:48<00:00,  1.17it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 658/658 [07:56<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [12:09<00:00,  1.08it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 650/650 [08:22<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 650/650 [07:48<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 945/945 [11:24<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 925/925 [12:02<00:00,  1.28it/s]\n",
            " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 554/842 [07:26<03:11,  1.51it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "for file_name in  x_eng_files:\n",
        "  lang_code = file_name.split(\"-\")[0].split(\"_\")[-1]\n",
        "  with open(base_path+file_name) as f:\n",
        "    ids=f.read().split()\n",
        "\n",
        "  if (lang_code in generated_translations.keys()) and (len(ids)==len(generated_translations[lang_code])) :\n",
        "    print(\"Done\")\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict[lang_code],split=\"test\",streaming=True,trust_remote_code=True)\n",
        "  except:\n",
        "    lang_issue.append(lang_code)\n",
        "    print(\"\\n Missing language \",lang_code)\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "  for item in tqdm(src_lang_data,total=len(ids)):\n",
        "    audio_sample = item['audio']\n",
        "    id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    if str(item['id'])+'_'+str(id) in generated_translations[lang_code]:\n",
        "      continue\n",
        "\n",
        "    if id not in ids:\n",
        "      continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Initially, try to process the audio on the GPU\n",
        "        audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\", sampling_rate=16000)\n",
        "        audio_inputs = {k: v.to('cuda') for k, v in audio_inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "        translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"\\nCUDA out of memory. Shifting inference to CPU for ID:\", id)\n",
        "            torch.cuda.empty_cache()  # Clear any unreleased memory\n",
        "\n",
        "            # Move the model to CPU for this inference\n",
        "            model.to('cpu')\n",
        "\n",
        "            try:\n",
        "                # Make sure audio_inputs are on the CPU as well\n",
        "                audio_inputs = {k: v.to('cpu') for k, v in audio_inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "                translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "            except Exception as cpu_e:\n",
        "                print(\"\\nFailed processing on CPU for ID:\", id, \"with error:\", cpu_e)\n",
        "                lang_missing_ids.append((lang_code, id))\n",
        "            finally:\n",
        "                # Regardless of the outcome, put the model back on the GPU for subsequent operations\n",
        "                model.to('cuda')\n",
        "        else:\n",
        "            print(\"\\nAn error occurred for ID:\", id, \"Error:\", e)\n",
        "            lang_missing_ids.append((lang_code, id))\n",
        "    except Exception as e:\n",
        "        print(\"\\nAn unexpected error occurred for ID:\", id, \"Error:\", e)\n",
        "        lang_missing_ids.append((lang_code, id))\n",
        "\n",
        "\n",
        "    del audio_inputs\n",
        "    del output_tokens\n",
        "\n",
        "    generated_translations[lang_code][str(item['id'])+'_'+str(id)]=translation\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  with open ('generated_large.json','w')as f:\n",
        "    json.dump(generated_translations,f,indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYrtUcIzFosB",
        "outputId": "81dfe5dd-e074-467c-d39b-514cd41163cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr8J1tTJWr-a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('generated_large.json') as f:\n",
        "    generated_translations = json.load(f)\n",
        "\n",
        "# Convert to defaultdict with empty dictionaries as default values\n",
        "generated_translations = defaultdict(dict, generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgW8fAoV3zde",
        "outputId": "be60cffb-bb53-42ed-f525-80d8cd746245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "eng_data=load_dataset('google/fleurs',name='en_us')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQkxs_E8NKDs"
      },
      "outputs": [],
      "source": [
        "eng_translation={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaStl6BdGOo6",
        "outputId": "26455541-135d-47c3-ad6a-7d7459e7d453"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:03<00:00, 765.20it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:00<00:00, 819.57it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 647/647 [00:00<00:00, 800.07it/s]\n"
          ]
        }
      ],
      "source": [
        "for split in eng_data:\n",
        "  for item in tqdm(eng_data[split]):\n",
        "      audio_sample = item['audio']\n",
        "      # id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "      eng_translation[item['id']]=item['raw_transcription']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkK9IWcfVN05"
      },
      "outputs": [],
      "source": [
        "seamless_fleurs_bleu={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfHbzMB6lDre"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nHGpy4fgGCB",
        "outputId": "133fbc3d-f1a5-4c3d-9bf9-f72f169748e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lang_missing_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G6Z_g7_PlaR"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "for lang_code in list(generated_translations.keys()):\n",
        "\n",
        "  translations=[]\n",
        "  gt_translations=[]\n",
        "\n",
        "  for i in generated_translations[lang_code]:\n",
        "      key=int(i.split('_')[0])\n",
        "      gt_translations.append(eng_translation[key])\n",
        "      translations.append(generated_translations[lang_code][i])\n",
        "\n",
        "  bleu = sacrebleu.corpus_bleu(translations, [gt_translations])\n",
        "  seamless_fleurs_bleu[lang_code]=round(bleu.score, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q-0FxJrXPv-",
        "outputId": "f38aced2-905c-4674-d3ea-5c8699ca145a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'afr': 39.69,\n",
              " 'amh': 17.034,\n",
              " 'arb': 31.725,\n",
              " 'asm': 17.47,\n",
              " 'ast': 25.894,\n",
              " 'azj': 16.425,\n",
              " 'bel': 16.056,\n",
              " 'ben': 22.778,\n",
              " 'bos': 32.891,\n",
              " 'bul': 31.33,\n",
              " 'cat': 37.574,\n",
              " 'ceb': 7.723,\n",
              " 'ces': 31.016,\n",
              " 'ckb': 20.487,\n",
              " 'cmn': 18.98,\n",
              " 'cym': 30.22,\n",
              " 'dan': 33.553,\n",
              " 'deu': 35.469,\n",
              " 'ell': 24.804,\n",
              " 'est': 28.534,\n",
              " 'fin': 25.782,\n",
              " 'fra': 32.641,\n",
              " 'ful': 0.788,\n",
              " 'gaz': 0.317,\n",
              " 'gle': 10.654,\n",
              " 'glg': 32.033,\n",
              " 'guj': 27.164,\n",
              " 'hau': 0.544,\n",
              " 'heb': 28.226,\n",
              " 'hin': 25.194,\n",
              " 'hrv': 29.8,\n",
              " 'hun': 24.166,\n",
              " 'hye': 27.81,\n",
              " 'ibo': 1.27,\n",
              " 'ind': 28.81,\n",
              " 'isl': 22.854,\n",
              " 'ita': 25.307,\n",
              " 'jav': 19.459,\n",
              " 'jpn': 15.886,\n",
              " 'kam': 1.803,\n",
              " 'kan': 21.799,\n",
              " 'kat': 18.741,\n",
              " 'kaz': 21.338,\n",
              " 'kea': 27.313,\n",
              " 'khk': 16.258,\n",
              " 'khm': 18.62,\n",
              " 'kir': 16.771,\n",
              " 'kor': 18.402,\n",
              " 'lao': 19.088,\n",
              " 'lin': 0.917,\n",
              " 'lit': 20.675,\n",
              " 'ltz': 14.429,\n",
              " 'lug': 16.179,\n",
              " 'luo': 0.789,\n",
              " 'lvs': 27.666,\n",
              " 'mal': 20.99,\n",
              " 'mar': 21.372,\n",
              " 'mkd': 33.972,\n",
              " 'mlt': 38.23,\n",
              " 'mri': 0.99,\n",
              " 'mya': 14.676,\n",
              " 'nld': 26.502,\n",
              " 'nob': 33.007,\n",
              " 'npi': 23.518,\n",
              " 'nso': 1.955,\n",
              " 'nya': 16.36,\n",
              " 'oci': 17.988,\n",
              " 'ory': 21.637,\n",
              " 'pan': 23.095,\n",
              " 'pbt': 11.242,\n",
              " 'pes': 27.74,\n",
              " 'pol': 22.163,\n",
              " 'por': 38.599,\n",
              " 'ron': 32.337,\n",
              " 'rus': 27.826,\n",
              " 'slk': 31.104,\n",
              " 'slv': 23.925,\n",
              " 'sna': 2.648,\n",
              " 'snd': 6.712,\n",
              " 'som': 14.259,\n",
              " 'spa': 24.544,\n",
              " 'srp': 35.158,\n",
              " 'swe': 33.979,\n",
              " 'swh': 25.754,\n",
              " 'tam': 15.136,\n",
              " 'tel': 20.319,\n",
              " 'tgk': 25.265,\n",
              " 'tgl': 21.896,\n",
              " 'tha': 18.013,\n",
              " 'tur': 24.819,\n",
              " 'ukr': 29.938,\n",
              " 'umb': 0.363,\n",
              " 'urd': 22.226,\n",
              " 'uzn': 21.475,\n",
              " 'vie': 20.481,\n",
              " 'wol': 0.832,\n",
              " 'xho': 3.126,\n",
              " 'yor': 12.487,\n",
              " 'yue': 12.381,\n",
              " 'zlm': 29.128,\n",
              " 'zul': 5.229}"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(sorted(seamless_fleurs_bleu.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i9Eat4-ldAe"
      },
      "outputs": [],
      "source": [
        "with open('seamless_large_fleurs_bleu.json','w')as f:\n",
        "  json.dump(seamless_fleurs_bleu,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at26E_5HqU2p",
        "outputId": "82cac807-ffeb-4a75-a78c-e763ec184d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "print(datasets.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJGFh4NA5lAg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}