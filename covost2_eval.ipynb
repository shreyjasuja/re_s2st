{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEv609HG0jkrzuQmiDxxZD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ij8C5NCMy_O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload openrc credentials file\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "import subprocess\n",
        "\n",
        "command = ['bash', '-c', 'source CHI-231138-openrc.sh && openstack container list']\n",
        "password = getpass(\"Please enter your password: \")  # Use getpass.getpass() to input this securely as shown above\n",
        "\n",
        "proc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "stdout, stderr = proc.communicate(input=password)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-BYPyC-bY9t",
        "outputId": "e47de788-4eb9-4449-9bb5-caa29f6eee10"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your password: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stdout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84JFm5QhcIu2",
        "outputId": "dc4ffe4e-6f0e-412e-ed08-2fa1ac66e487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(sj4020@nyu.edu) Please enter your Chameleon CLI password: \n",
            "+-----------------------+\n",
            "| Name                  |\n",
            "+-----------------------+\n",
            "| CoVoST2_data          |\n",
            "| CoVoST2_data_segments |\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd data && chmod +x extract_and_cleanup.sh && ./extract_and_cleanup.sh"
      ],
      "metadata": {
        "id": "WFTnq59Ke6le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -y install ffmpeg"
      ],
      "metadata": {
        "id": "CVxDwm2JpIHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add some examples of languages\n"
      ],
      "metadata": {
        "id": "jxQGPannUOqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###dividing the language categories"
      ],
      "metadata": {
        "id": "f4CZ7zYNUAEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Whisper and do inference on CoVoST2 dataset"
      ],
      "metadata": {
        "id": "6AY5FCQD1RuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"large-v2\")"
      ],
      "metadata": {
        "id": "wNa6oB8WofoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWcM0iMlplvZ",
        "outputId": "f40f2038-3154-4b55-e6d1-cd02d3a636b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is multilingual and has 1,541,384,960 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_levels=[\"low_res\",\"mid_res\",\"high_res\"]"
      ],
      "metadata": {
        "id": "JCSXrTO5ug9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_res=['ca','de','fr','es']\n",
        "mid_res=['zh-CN','fa','it','ru','pt']\n",
        "low_res=['mn','ta','lv','et','cy','sl','ja','tr','ar','nl','sv-SE','id']\n"
      ],
      "metadata": {
        "id": "Rx2DhJY3jG6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "provide refrence to the options set up in transcribe functions, why these choices"
      ],
      "metadata": {
        "id": "H6zRjGcQUx5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def whisper_inference(src_lang):\n",
        "  x_en=load_dataset(\"covost2\",src_lang+\"_en\",data_dir=\"data/\"+src_lang,split=\"test\",trust_remote_code=True)\n",
        "\n",
        "  options = dict(language=src_lang.split('-')[0], beam_size=5, best_of=5)\n",
        "  # transcribe_options = dict(task=\"transcribe\",**options))\n",
        "  translate_options = dict(task=\"translate\",**options)\n",
        "\n",
        "  translations = []\n",
        "  gt_translations = []\n",
        "\n",
        "  # transcriptions = []\n",
        "  # gt_transcripts=[]\n",
        "\n",
        "\n",
        "  for item in tqdm(x_en):\n",
        "      audio = item['file']\n",
        "\n",
        "      translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
        "      translations.append(translation)\n",
        "      gt_translations.append(item['translation'])\n",
        "\n",
        "      # transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "      # transcriptions.append(transcription)\n",
        "      # gt_transcripts.append(item['sentence'])\n",
        "  return translations, gt_translations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sSaBRfdy1mfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate over the dataset"
      ],
      "metadata": {
        "id": "S98tDjFSWacs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_sacre_bleu(translations,gt_translations):\n",
        "  #calculate BLEU score\n",
        "  bleu = sacrebleu.corpus_bleu(translations, [gt_translations])\n",
        "  return round(bleu.score, 3)"
      ],
      "metadata": {
        "id": "RR_N23BUWZGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "2rZylXaktmBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "def evaluate_nltk_bleu(translations,gt_translations):\n",
        "  references = [[word_tokenize(ref)] for ref in gt_translations]\n",
        "  candidates = [word_tokenize(cand) for cand in translations]\n",
        "  bleu_score=corpus_bleu(list_of_references=references,hypotheses=candidates)\n",
        "  return round(bleu_score * 100, 3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zPx8JkeGml-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_bleu_score"
      ],
      "metadata": {
        "id": "C5Caui1cP4jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sacre_bleu_score=collections.defaultdict(dict)"
      ],
      "metadata": {
        "id": "EjE17ileSj_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tGSG328iA2R",
        "outputId": "ad1007db-3ef5-4aee-ff01-b3ccc27bb319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fr_en': 35.45284836988789,\n",
              " 'de_en': 34.88597687724918,\n",
              " 'es_en': 39.55997842639303,\n",
              " 'ca_en': 30.759666441449568,\n",
              " 'it_en': 36.06574058977499,\n",
              " 'ru_en': 42.25683321797073,\n",
              " 'zh-CN_en': 15.964298794839507,\n",
              " 'pt_en': 50.95443524854168,\n",
              " 'fa_en': 17.683204989908223,\n",
              " 'et_en': 12.716616538768118,\n",
              " 'mn_en': 0.1362710856910444,\n",
              " 'nl_en': 40.05971133578579,\n",
              " 'tr_en': 27.22795731861111,\n",
              " 'ar_en': 37.94387040560137,\n",
              " 'sv-SE_en': 41.77001588858687,\n",
              " 'lv_en': 12.570124272077669,\n",
              " 'sl_en': 19.459277620216994,\n",
              " 'ta_en': 3.7484385576194366,\n",
              " 'ja_en': 24.570849078756105,\n",
              " 'id_en': 46.600081176881766,\n",
              " 'cy_en': 19.088220048271875}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the DataFrame to store results\n",
        "columns = ['Language Code', 'Transcript', 'Ground Truth Transcript', 'Translation', 'Ground Truth Translation']\n",
        "results_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "\n",
        "for code in lang_codes:\n",
        "  transcripts, gt_transcripts, translations, gt_translations= inference(code)\n",
        "\n",
        "   # Creating a temporary DataFrame from the current inference results\n",
        "  temp_df = pd.DataFrame({\n",
        "        'Language Code': [code] * len(transcripts),\n",
        "        'Transcript': transcripts,\n",
        "        'Ground Truth Transcript': gt_transcripts,\n",
        "        'Translation': translations,\n",
        "        'Ground Truth Translation': gt_translations\n",
        "  })\n",
        "\n",
        "  # Appending the results to the main DataFrame\n",
        "  results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr5r8BrYezFL",
        "outputId": "447a8d0b-bc29-48ae-8368-7edcf339746f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 6/14760 [00:13<8:59:26,  2.19s/it]\n",
            "  0%|          | 6/13511 [00:17<10:57:55,  2.92s/it]\n",
            "  0%|          | 6/13221 [00:12<7:53:23,  2.15s/it]\n",
            "  0%|          | 6/12730 [00:29<17:09:15,  4.85s/it]\n",
            "  0%|          | 6/8951 [00:14<6:11:39,  2.49s/it]\n",
            "  0%|          | 6/6300 [00:14<4:15:42,  2.44s/it]\n",
            "  0%|          | 6/4898 [00:32<7:27:20,  5.49s/it]\n",
            "  0%|          | 6/4023 [00:12<2:22:36,  2.13s/it]\n",
            "  0%|          | 6/3445 [00:13<2:10:40,  2.28s/it]\n",
            "  0%|          | 6/1571 [00:34<2:31:26,  5.81s/it]\n",
            "  0%|          | 6/1759 [01:35<7:47:00, 15.98s/it]\n",
            "  0%|          | 6/1699 [00:13<1:04:50,  2.30s/it]\n",
            "  0%|          | 6/1629 [00:58<4:24:49,  9.79s/it]\n",
            "  0%|          | 6/1695 [00:12<59:49,  2.13s/it]\n",
            "  0%|          | 6/1595 [00:10<45:17,  1.71s/it]\n",
            "  0%|          | 6/1629 [00:24<1:48:22,  4.01s/it]\n",
            "  2%|▏         | 6/360 [00:17<17:22,  2.94s/it]\n",
            "  1%|          | 6/786 [00:21<47:19,  3.64s/it]\n",
            "  1%|          | 6/684 [00:17<32:52,  2.91s/it]\n",
            "  1%|          | 6/844 [00:11<25:52,  1.85s/it]\n",
            "  1%|          | 6/690 [00:19<37:06,  3.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('whisper_results.csv')"
      ],
      "metadata": {
        "id": "ZLWW3-4Flv1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "UIZncuqJngdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XLS-R (2b)"
      ],
      "metadata": {
        "id": "ZYrhKqeBeU_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Optionally, you can also check the GPU memory status afterwards\n",
        "# This step is just for confirmation and not necessary for the operation\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory}\")\n",
        "    print(f\"Available GPU Memory: {torch.cuda.memory_reserved(0)}\")\n",
        "    print(f\"Allocated GPU Memory: {torch.cuda.memory_allocated(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjEUX4RLntva",
        "outputId": "46e20b0a-7aef-4026-fd55-d3e886f2924e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GPU Memory: 25388515328\n",
            "Available GPU Memory: 0\n",
            "Allocated GPU Memory: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "ydeqyUaul46J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import SpeechEncoderDecoderModel,MBart50Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAboaE2kgyyG",
        "outputId": "20c8bcea-4502-4ad3-a7fc-63267882cd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(\"facebook/wav2vec2-xls-r-2b-21-to-en\")"
      ],
      "metadata": {
        "id": "i09XNcgD-3h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "AwwCHQ6UZnRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "asr=pipeline(model=\"facebook/wav2vec2-xls-r-2b-21-to-en\",tokenizer=tokenizer,feature_extractor=feature_extractor,device=0)"
      ],
      "metadata": {
        "id": "wnWbf-JEICo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xlsr_inference(src_lang):\n",
        "  x_en=load_dataset(\"covost2\",src_lang+\"_en\",data_dir=\"data/\"+src_lang,split=\"test\",trust_remote_code=True)\n",
        "\n",
        "  translations = []\n",
        "  gt_translations = []\n",
        "\n",
        "  # transcriptions = []\n",
        "  # gt_transcripts=[]\n",
        "\n",
        "\n",
        "  for item in tqdm(x_en):\n",
        "      audio = item['file']\n",
        "\n",
        "      translation = asr(audio)[\"text\"]\n",
        "      translations.append(translation)\n",
        "      gt_translations.append(item['translation'])\n",
        "\n",
        "      # transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "      # transcriptions.append(transcription)\n",
        "      # gt_transcripts.append(item['sentence'])\n",
        "  return translations, gt_translations"
      ],
      "metadata": {
        "id": "RRwFVWA1pUte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "xlsr_bleu_score=collections.defaultdict(dict)"
      ],
      "metadata": {
        "id": "TiZPjlBJXDD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "file_name = 'xlsr_nltk_bleu.json'\n",
        "for i in res_levels:\n",
        "  for src in eval(i):\n",
        "    translations, ground_truth=xlsr_inference(src)\n",
        "    xlsr_bleu_score[i][src]=evaluate_nltk_bleu(translations=translations,gt_translations=ground_truth)\n",
        "    with open(file_name, 'w') as f:\n",
        "      json.dump(xlsr_bleu_score, f, indent=4)"
      ],
      "metadata": {
        "id": "ty8crgbd_bfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlsr_bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpyW1Gmx_jRV",
        "outputId": "37850e5c-2347-4b59-8c56-ae0cdcf2c5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(dict,\n",
              "            {'low_res': {'mn': 1.877,\n",
              "              'ta': 0.613,\n",
              "              'lv': 20.774,\n",
              "              'et': 11.186,\n",
              "              'cy': 14.671,\n",
              "              'sl': 19.117,\n",
              "              'ja': 4.102,\n",
              "              'tr': 16.774,\n",
              "              'ar': 16.991,\n",
              "              'nl': 31.883,\n",
              "              'sv-SE': 30.987,\n",
              "              'id': 16.255},\n",
              "             'mid_res': {'zh-CN': 9.475,\n",
              "              'fa': 13.073,\n",
              "              'it': 35.034,\n",
              "              'ru': 39.44,\n",
              "              'pt': 42.012},\n",
              "             'high_res': {'ca': 33.813,\n",
              "              'de': 33.486,\n",
              "              'fr': 37.614,\n",
              "              'es': 39.166}})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seamless medium on CoVoST 2 data"
      ],
      "metadata": {
        "id": "CR5TdYiQnS6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")"
      ],
      "metadata": {
        "id": "u1YkQLMTuF8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ffbbaf-a527-47f4-d6cf-b274ebba11b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/cc/.local/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "id": "vNt_E2Nwu29J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seamless_inference(src_lang):\n",
        "  x_en=load_dataset(\"covost2\",src_lang+\"_en\",data_dir=\"data/\"+src_lang,split=\"test\",trust_remote_code=True)\n",
        "\n",
        "  translations = []\n",
        "  gt_translations = []\n",
        "\n",
        "  # transcriptions = []\n",
        "  # gt_transcripts=[]\n",
        "\n",
        "\n",
        "  for item in tqdm(x_en):\n",
        "      audio_sample = item['audio']\n",
        "      audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\",sampling_rate=16000)\n",
        "      audio_inputs = {k: v.to('cuda') for k, v in audio_inputs.items()}\n",
        "\n",
        "      output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\",generate_speech=False)\n",
        "      translation=processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "      translations.append(translation)\n",
        "      gt_translations.append(item['translation'])\n",
        "\n",
        "      # transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "      # transcriptions.append(transcription)\n",
        "      # gt_transcripts.append(item['sentence'])\n",
        "  return translations, gt_translations"
      ],
      "metadata": {
        "id": "01h2MNzOnibE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "import collections\n",
        "seamless_bleu_score=collections.defaultdict(dict)"
      ],
      "metadata": {
        "id": "WDEWrOs1pzYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "file_name = 'seamless_large_sacre_bleu.json'\n",
        "for i in res_levels:\n",
        "  for src in eval(i):\n",
        "    translations, ground_truth=seamless_inference(src)\n",
        "    seamless_bleu_score[i][src]=evaluate_sacre_bleu(translations=translations,gt_translations=ground_truth)\n",
        "    with open(file_name, 'w') as f:\n",
        "      json.dump(seamless_bleu_score, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLJm88V5t1DY",
        "outputId": "7ce40f6a-d72a-4bf1-e4d6-042c1666e56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1759/1759 [12:11<00:00,  2.41it/s]\n",
            "100%|██████████| 786/786 [03:29<00:00,  3.75it/s]\n",
            "100%|██████████| 1629/1629 [06:33<00:00,  4.14it/s]\n",
            "100%|██████████| 1571/1571 [13:28<00:00,  1.94it/s]\n",
            "100%|██████████| 690/690 [03:42<00:00,  3.10it/s]\n",
            "100%|██████████| 360/360 [01:46<00:00,  3.38it/s]\n",
            "100%|██████████| 684/684 [03:21<00:00,  3.40it/s]\n",
            "100%|██████████| 1629/1629 [08:26<00:00,  3.21it/s]\n",
            "100%|██████████| 1695/1695 [07:31<00:00,  3.76it/s]\n",
            "100%|██████████| 1699/1699 [08:25<00:00,  3.36it/s]\n",
            "100%|██████████| 1595/1595 [06:42<00:00,  3.97it/s]\n",
            "100%|██████████| 844/844 [03:24<00:00,  4.12it/s]\n",
            "100%|██████████| 4898/4898 [32:36<00:00,  2.50it/s]\n",
            "100%|██████████| 3445/3445 [17:31<00:00,  3.28it/s]\n",
            "100%|██████████| 8951/8951 [56:18<00:00,  2.65it/s]\n",
            "100%|██████████| 6300/6300 [41:09<00:00,  2.55it/s]\n",
            "100%|██████████| 4023/4023 [20:34<00:00,  3.26it/s]\n",
            "100%|██████████| 12730/12730 [1:18:42<00:00,  2.70it/s]\n",
            " 50%|█████     | 6766/13511 [40:45<37:00,  3.04it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seamless_bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTlATNFoUvm7",
        "outputId": "3db0ba9f-08ad-444c-a006-b58a4313958f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(dict,\n",
              "            {'low_res': {'mn': 7.378,\n",
              "              'ta': 3.931,\n",
              "              'lv': 26.902,\n",
              "              'et': 26.298,\n",
              "              'cy': 55.276,\n",
              "              'sl': 38.32,\n",
              "              'ja': 19.69,\n",
              "              'tr': 30.647,\n",
              "              'ar': 45.732,\n",
              "              'nl': 40.112,\n",
              "              'sv-SE': 37.972,\n",
              "              'id': 50.455},\n",
              "             'mid_res': {'zh-CN': 19.911,\n",
              "              'fa': 25.213,\n",
              "              'it': 38.805,\n",
              "              'ru': 47.881,\n",
              "              'pt': 49.055},\n",
              "             'high_res': {'ca': 37.969,\n",
              "              'de': 38.009,\n",
              "              'fr': 40.724,\n",
              "              'es': 40.639}})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s=0\n",
        "for i in res_levels:\n",
        "  for k in eval(i):\n",
        "    s+=seamless_bleu_score[i][k]"
      ],
      "metadata": {
        "id": "APlPgV4XuV41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s/21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s8cjKrsvTJ6",
        "outputId": "f8e7af91-b0e7-4d27-a8ee-43e22a551336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31.29719047619048"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s/21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3q56_7EvbsR",
        "outputId": "ce2492ba-ce99-4217-a895-1ff281bd9873"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.32947619047619"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLEURS DATASET"
      ],
      "metadata": {
        "id": "GcjLYl0FU1pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "LzSLzRZ3pZpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pycountry import languages\n",
        "def bcp47_to_iso639_3(bcp47_code):\n",
        "    parts = bcp47_code.split('_')\n",
        "    lang_code = parts[0]\n",
        "    try:\n",
        "\n",
        "      lang = languages.get(alpha_2=lang_code).alpha_3\n",
        "      return lang.lower()\n",
        "    except (AttributeError, KeyError) as e:\n",
        "        # If the mapping fails, return the original code\n",
        "        print(e, lang_code)\n",
        "        return lang_code.lower()"
      ],
      "metadata": {
        "id": "G66B1hRoU1QI"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "bcp_47_codes=get_dataset_config_names(\"google/fleurs\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "OHIYGM2yjNOr"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_dict={}\n",
        "for i in bcp_47_codes:\n",
        "  lang_dict[bcp47_to_iso639_3(i)]=i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LFdNikSU9UL",
        "outputId": "34e9b3c7-01f6-4c30-bf1a-ef9f8df31190"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'NoneType' object has no attribute 'alpha_3' ast\n",
            "'NoneType' object has no attribute 'alpha_3' ceb\n",
            "'NoneType' object has no attribute 'alpha_3' ckb\n",
            "'NoneType' object has no attribute 'alpha_3' cmn\n",
            "'NoneType' object has no attribute 'alpha_3' fil\n",
            "'NoneType' object has no attribute 'alpha_3' kam\n",
            "'NoneType' object has no attribute 'alpha_3' kea\n",
            "'NoneType' object has no attribute 'alpha_3' luo\n",
            "'NoneType' object has no attribute 'alpha_3' nso\n",
            "'NoneType' object has no attribute 'alpha_3' umb\n",
            "'NoneType' object has no attribute 'alpha_3' yue\n",
            "'NoneType' object has no attribute 'alpha_3' all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXNZiPrUbZYi",
        "outputId": "2ccdd99c-5a54-4dca-ca41-32d704c18973"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-28 06:01:33--  https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip\r\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.19, 3.162.163.11, 3.162.163.51, ...\r\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6096377 (5.8M) [application/zip]\n",
            "Saving to: ‘evaluation_data_ids.zip’\n",
            "\n",
            "evaluation_data_ids 100%[===================>]   5.81M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-03-28 06:01:33 (79.2 MB/s) - ‘evaluation_data_ids.zip’ saved [6096377/6096377]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install unzip\n",
        "!unzip evaluation_data_ids.zip && rm evaluation_data_ids.zip"
      ],
      "metadata": {
        "id": "hAe8UrAHneNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_codes=['msa','fil','uzb','fas','nep','lav','ara','aze','pus','ori','mon','swa','orm']\n",
        "new_codes=['zlm','tgl','uzn','pes','npi','lvs','arb','azj','pbt','ory','khk','swh','gaz']\n",
        "for i in range(len(new_codes)):\n",
        "  lang_dict[new_codes[i]]=lang_dict[old_codes[i]]\n",
        "  del lang_dict[old_codes[i]]"
      ],
      "metadata": {
        "id": "p2I1pSjmqi7_"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path=\"evaluation_data_ids/s2tt_fleurs_ids/\"\n",
        "x_eng_files = [file for file in os.listdir(\"evaluation_data_ids/s2tt_fleurs_ids/\") if file.endswith('-eng.ids')]\n",
        "print(len(x_eng_files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686SAdR3shmC",
        "outputId": "261377f8-f579-4a67-eac1-789c6b93ebb8"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "generated_translations=collections.defaultdict(dict)"
      ],
      "metadata": {
        "id": "rFOtLKC1nhfw"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict['ukr'],split=\"test\",streaming=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZYl81tzoHjK",
        "outputId": "36e04359-92a4-4dd8-830e-eba0cd8b4c4b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(iter(src_lang_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XtWrSCg1Cj0",
        "outputId": "747e716c-8981-4fe7-f309-80742168be92"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 1982, 'num_samples': 118080, 'path': None, 'audio': {'path': 'test/10021730821550109934.wav', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.80006027e-05, -2.40206718e-05, -9.59634781e-05]), 'sampling_rate': 16000}, 'transcription': 'жінки усім подорожнім жінкам радять казати що вони заміжні незалежно від справжнього сімейного стану', 'raw_transcription': 'Жінки: усім подорожнім жінкам радять казати, що вони заміжні, незалежно від справжнього сімейного стану.', 'gender': 0, 'lang_id': 92, 'language': 'Ukrainian', 'lang_group_id': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "X37Ho9kPT9zV"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_issue=[]\n",
        "lang_missing_ids=[]"
      ],
      "metadata": {
        "id": "fWWb2q8vPCms"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "for file_name in  x_eng_files:\n",
        "  lang_code = file_name.split(\"-\")[0].split(\"_\")[-1]\n",
        "  with open(base_path+file_name) as f:\n",
        "    ids=f.read().split()\n",
        "\n",
        "  if (lang_code in generated_translations.keys()) and (len(ids)==len(generated_translations[lang_code])) :\n",
        "    print(\"Done\")\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict[lang_code],split=\"test\",streaming=True,trust_remote_code=True)\n",
        "  except:\n",
        "    lang_issue.append(lang_code)\n",
        "    print(\"\\n Missing language \",lang_code)\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "  for item in tqdm(src_lang_data,total=len(ids)):\n",
        "    audio_sample = item['audio']\n",
        "    id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    if str(item['id'])+'_'+str(id) in generated_translations[lang_code]:\n",
        "      continue\n",
        "\n",
        "    if id not in ids:\n",
        "      continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Initially, try to process the audio on the GPU\n",
        "        audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\", sampling_rate=16000)\n",
        "        audio_inputs = {k: v.to('cuda') for k, v in audio_inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "        translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"\\nCUDA out of memory. Shifting inference to CPU for ID:\", id)\n",
        "            torch.cuda.empty_cache()  # Clear any unreleased memory\n",
        "\n",
        "            # Move the model to CPU for this inference\n",
        "            model.to('cpu')\n",
        "\n",
        "            try:\n",
        "                # Make sure audio_inputs are on the CPU as well\n",
        "                audio_inputs = {k: v.to('cpu') for k, v in audio_inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "                translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "            except Exception as cpu_e:\n",
        "                print(\"\\nFailed processing on CPU for ID:\", id, \"with error:\", cpu_e)\n",
        "                lang_missing_ids.append((lang_code, id))\n",
        "            finally:\n",
        "                # Regardless of the outcome, put the model back on the GPU for subsequent operations\n",
        "                model.to('cuda')\n",
        "        else:\n",
        "            print(\"\\nAn error occurred for ID:\", id, \"Error:\", e)\n",
        "            lang_missing_ids.append((lang_code, id))\n",
        "    except Exception as e:\n",
        "        print(\"\\nAn unexpected error occurred for ID:\", id, \"Error:\", e)\n",
        "        lang_missing_ids.append((lang_code, id))\n",
        "\n",
        "\n",
        "    del audio_inputs\n",
        "    del output_tokens\n",
        "\n",
        "    generated_translations[lang_code][str(item['id'])+'_'+str(id)]=translation\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  with open ('generated_large.json','w')as f:\n",
        "    json.dump(generated_translations,f,indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p6PX1kn0xBa",
        "outputId": "020e7e09-9ee3-4938-ee6d-c2cda0d4b139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 750/750 [09:11<00:00,  1.36it/s]\n",
            "100%|██████████| 908/908 [11:45<00:00,  1.29it/s]\n",
            "100%|██████████| 926/926 [12:11<00:00,  1.27it/s]\n",
            "100%|██████████| 728/728 [08:41<00:00,  1.40it/s]\n",
            "100%|██████████| 749/749 [08:40<00:00,  1.44it/s]\n",
            "100%|██████████| 1015/1015 [12:48<00:00,  1.32it/s]\n",
            "264it [03:08,  1.40it/s]\n",
            "100%|██████████| 1041/1041 [15:38<00:00,  1.11it/s]\n",
            "100%|██████████| 918/918 [11:43<00:00,  1.31it/s]\n",
            "100%|██████████| 357/357 [04:26<00:00,  1.34it/s]\n",
            "100%|██████████| 1021/1021 [13:00<00:00,  1.31it/s]\n",
            "100%|██████████| 880/880 [11:24<00:00,  1.29it/s]\n",
            "100%|██████████| 946/946 [11:01<00:00,  1.43it/s]\n",
            "100%|██████████| 977/977 [11:28<00:00,  1.42it/s]\n",
            "100%|██████████| 687/687 [08:17<00:00,  1.38it/s]\n",
            "100%|██████████| 857/857 [10:27<00:00,  1.37it/s]\n",
            "100%|██████████| 660/660 [11:18<00:00,  1.03s/it]\n",
            "100%|██████████| 621/621 [09:49<00:00,  1.05it/s]\n",
            "100%|██████████| 792/792 [09:20<00:00,  1.41it/s]\n",
            "100%|██████████| 925/925 [15:11<00:00,  1.01it/s]\n",
            "100%|██████████| 1008/1008 [17:31<00:00,  1.04s/it]\n",
            "100%|██████████| 893/893 [11:12<00:00,  1.33it/s]\n",
            "100%|██████████| 591/591 [07:18<00:00,  1.35it/s]\n",
            " 63%|██████▎   | 237/379 [03:57<02:31,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 3189556219205510204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 312/379 [05:56<00:58,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 6634898757415929965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 379/379 [08:08<00:00,  1.29s/it]\n",
            "100%|██████████| 905/905 [11:21<00:00,  1.33it/s]\n",
            "100%|██████████| 964/964 [13:15<00:00,  1.21it/s]\n",
            "100%|██████████| 919/919 [11:39<00:00,  1.31it/s]\n",
            "100%|██████████| 920/920 [11:09<00:00,  1.38it/s]\n",
            "100%|██████████| 831/831 [10:39<00:00,  1.30it/s]\n",
            "100%|██████████| 771/771 [10:06<00:00,  1.27it/s]\n",
            "100%|██████████| 364/364 [04:10<00:00,  1.45it/s]\n",
            "100%|██████████| 979/979 [12:03<00:00,  1.35it/s]\n",
            "100%|██████████| 854/854 [12:44<00:00,  1.12it/s]\n",
            "100%|██████████| 980/980 [12:53<00:00,  1.27it/s]\n",
            "100%|██████████| 1019/1019 [13:19<00:00,  1.27it/s]\n",
            "100%|██████████| 371/371 [05:31<00:00,  1.12it/s]\n",
            "100%|██████████| 723/723 [09:26<00:00,  1.28it/s]\n",
            "100%|██████████| 1021/1021 [13:15<00:00,  1.28it/s]\n",
            "100%|██████████| 862/862 [10:54<00:00,  1.32it/s]\n",
            "100%|██████████| 299/299 [03:40<00:00,  1.36it/s]\n",
            "100%|██████████| 871/871 [11:20<00:00,  1.28it/s]\n",
            "100%|██████████| 743/743 [09:15<00:00,  1.34it/s]\n",
            "100%|██████████| 1000/1000 [11:28<00:00,  1.45it/s]\n",
            "100%|██████████| 838/838 [10:20<00:00,  1.35it/s]\n",
            "100%|██████████| 969/969 [12:56<00:00,  1.25it/s]\n",
            "100%|██████████| 574/574 [06:31<00:00,  1.47it/s]\n",
            "100%|██████████| 726/726 [08:38<00:00,  1.40it/s]\n",
            "100%|██████████| 883/883 [10:43<00:00,  1.37it/s]\n",
            "100%|██████████| 792/792 [09:56<00:00,  1.33it/s]\n",
            "100%|██████████| 856/856 [11:18<00:00,  1.26it/s]\n",
            "100%|██████████| 862/862 [11:09<00:00,  1.29it/s]\n",
            "100%|██████████| 958/958 [11:56<00:00,  1.34it/s]\n",
            "100%|██████████| 851/851 [10:53<00:00,  1.30it/s]\n",
            "100%|██████████| 923/923 [12:15<00:00,  1.25it/s]\n",
            "100%|██████████| 922/922 [10:48<00:00,  1.42it/s]\n",
            "100%|██████████| 428/428 [05:02<00:00,  1.41it/s]\n",
            "100%|██████████| 834/834 [10:17<00:00,  1.35it/s]\n",
            "100%|██████████| 600/600 [07:56<00:00,  1.26it/s]\n",
            " 14%|█▎        | 55/405 [00:42<04:34,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 12560373056138365189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [07:16<00:00,  1.08s/it]\n",
            "100%|██████████| 984/984 [11:58<00:00,  1.37it/s]\n",
            "100%|██████████| 761/761 [10:01<00:00,  1.26it/s]\n",
            "100%|██████████| 759/759 [09:22<00:00,  1.35it/s]\n",
            "100%|██████████| 512/512 [06:43<00:00,  1.27it/s]\n",
            "100%|██████████| 998/998 [14:30<00:00,  1.15it/s]\n",
            "100%|██████████| 478/478 [08:00<00:00,  1.00s/it]\n",
            "100%|██████████| 865/865 [11:54<00:00,  1.21it/s]\n",
            "100%|██████████| 973/973 [12:13<00:00,  1.33it/s]\n",
            "100%|██████████| 382/382 [04:34<00:00,  1.39it/s]\n",
            "100%|██████████| 723/723 [08:57<00:00,  1.35it/s]\n",
            "100%|██████████| 932/932 [11:28<00:00,  1.35it/s]\n",
            "100%|██████████| 927/927 [11:11<00:00,  1.38it/s]\n",
            "100%|██████████| 676/676 [08:19<00:00,  1.35it/s]\n",
            "100%|██████████| 883/883 [10:34<00:00,  1.39it/s]\n",
            "100%|██████████| 827/827 [13:04<00:00,  1.05it/s]\n",
            "100%|██████████| 949/949 [11:16<00:00,  1.40it/s]\n",
            "100%|██████████| 46/46 [00:39<00:00,  1.16it/s]\n",
            "100%|██████████| 541/541 [07:16<00:00,  1.24it/s]\n",
            "100%|██████████| 914/914 [11:13<00:00,  1.36it/s]\n",
            "100%|██████████| 775/775 [09:55<00:00,  1.30it/s]\n",
            "100%|██████████| 819/819 [10:04<00:00,  1.36it/s]\n",
            "100%|██████████| 256/256 [04:19<00:00,  1.01s/it]\n",
            "100%|██████████| 487/487 [06:12<00:00,  1.31it/s]\n",
            "100%|██████████| 758/758 [08:53<00:00,  1.42it/s]\n",
            "100%|██████████| 940/940 [11:49<00:00,  1.33it/s]\n",
            "100%|██████████| 986/986 [12:15<00:00,  1.34it/s]\n",
            "100%|██████████| 930/930 [11:23<00:00,  1.36it/s]\n",
            "100%|██████████| 41/41 [00:36<00:00,  1.11it/s]\n",
            "100%|██████████| 934/934 [11:19<00:00,  1.37it/s]\n",
            "100%|██████████| 418/418 [04:59<00:00,  1.40it/s]\n",
            "100%|██████████| 472/472 [05:40<00:00,  1.39it/s]\n",
            "100%|██████████| 700/700 [08:36<00:00,  1.36it/s]\n",
            "100%|██████████| 864/864 [10:51<00:00,  1.33it/s]\n",
            "100%|██████████| 967/967 [13:48<00:00,  1.17it/s]\n",
            "100%|██████████| 658/658 [07:56<00:00,  1.38it/s]\n",
            "100%|██████████| 790/790 [12:09<00:00,  1.08it/s]\n",
            "100%|██████████| 650/650 [08:22<00:00,  1.29it/s]\n",
            "100%|██████████| 650/650 [07:48<00:00,  1.39it/s]\n",
            "100%|██████████| 945/945 [11:24<00:00,  1.38it/s]\n",
            "100%|██████████| 925/925 [12:02<00:00,  1.28it/s]\n",
            " 66%|██████▌   | 554/842 [07:26<03:11,  1.51it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(generated_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYrtUcIzFosB",
        "outputId": "81dfe5dd-e074-467c-d39b-514cd41163cf"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('generated_large.json') as f:\n",
        "    generated_translations = json.load(f)\n",
        "\n",
        "# Convert to defaultdict with empty dictionaries as default values\n",
        "generated_translations = defaultdict(dict, generated_translations)"
      ],
      "metadata": {
        "id": "Dr8J1tTJWr-a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_data=load_dataset('google/fleurs',name='en_us')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgW8fAoV3zde",
        "outputId": "be60cffb-bb53-42ed-f525-80d8cd746245"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_translation={}"
      ],
      "metadata": {
        "id": "tQkxs_E8NKDs"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in eng_data:\n",
        "  for item in tqdm(eng_data[split]):\n",
        "      audio_sample = item['audio']\n",
        "      # id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "      eng_translation[item['id']]=item['raw_transcription']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaStl6BdGOo6",
        "outputId": "26455541-135d-47c3-ad6a-7d7459e7d453"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2602/2602 [00:03<00:00, 765.20it/s]\n",
            "100%|██████████| 394/394 [00:00<00:00, 819.57it/s]\n",
            "100%|██████████| 647/647 [00:00<00:00, 800.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seamless_fleurs_bleu={}"
      ],
      "metadata": {
        "id": "mkK9IWcfVN05"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZfHbzMB6lDre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_missing_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nHGpy4fgGCB",
        "outputId": "133fbc3d-f1a5-4c3d-9bf9-f72f169748e9"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "for lang_code in list(generated_translations.keys()):\n",
        "\n",
        "  translations=[]\n",
        "  gt_translations=[]\n",
        "\n",
        "  for i in generated_translations[lang_code]:\n",
        "      key=int(i.split('_')[0])\n",
        "      gt_translations.append(eng_translation[key])\n",
        "      translations.append(generated_translations[lang_code][i])\n",
        "\n",
        "  bleu = sacrebleu.corpus_bleu(translations, [gt_translations])\n",
        "  seamless_fleurs_bleu[lang_code]=round(bleu.score, 3)"
      ],
      "metadata": {
        "id": "4G6Z_g7_PlaR"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(sorted(seamless_fleurs_bleu.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q-0FxJrXPv-",
        "outputId": "f38aced2-905c-4674-d3ea-5c8699ca145a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'afr': 39.69,\n",
              " 'amh': 17.034,\n",
              " 'arb': 31.725,\n",
              " 'asm': 17.47,\n",
              " 'ast': 25.894,\n",
              " 'azj': 16.425,\n",
              " 'bel': 16.056,\n",
              " 'ben': 22.778,\n",
              " 'bos': 32.891,\n",
              " 'bul': 31.33,\n",
              " 'cat': 37.574,\n",
              " 'ceb': 7.723,\n",
              " 'ces': 31.016,\n",
              " 'ckb': 20.487,\n",
              " 'cmn': 18.98,\n",
              " 'cym': 30.22,\n",
              " 'dan': 33.553,\n",
              " 'deu': 35.469,\n",
              " 'ell': 24.804,\n",
              " 'est': 28.534,\n",
              " 'fin': 25.782,\n",
              " 'fra': 32.641,\n",
              " 'ful': 0.788,\n",
              " 'gaz': 0.317,\n",
              " 'gle': 10.654,\n",
              " 'glg': 32.033,\n",
              " 'guj': 27.164,\n",
              " 'hau': 0.544,\n",
              " 'heb': 28.226,\n",
              " 'hin': 25.194,\n",
              " 'hrv': 29.8,\n",
              " 'hun': 24.166,\n",
              " 'hye': 27.81,\n",
              " 'ibo': 1.27,\n",
              " 'ind': 28.81,\n",
              " 'isl': 22.854,\n",
              " 'ita': 25.307,\n",
              " 'jav': 19.459,\n",
              " 'jpn': 15.886,\n",
              " 'kam': 1.803,\n",
              " 'kan': 21.799,\n",
              " 'kat': 18.741,\n",
              " 'kaz': 21.338,\n",
              " 'kea': 27.313,\n",
              " 'khk': 16.258,\n",
              " 'khm': 18.62,\n",
              " 'kir': 16.771,\n",
              " 'kor': 18.402,\n",
              " 'lao': 19.088,\n",
              " 'lin': 0.917,\n",
              " 'lit': 20.675,\n",
              " 'ltz': 14.429,\n",
              " 'lug': 16.179,\n",
              " 'luo': 0.789,\n",
              " 'lvs': 27.666,\n",
              " 'mal': 20.99,\n",
              " 'mar': 21.372,\n",
              " 'mkd': 33.972,\n",
              " 'mlt': 38.23,\n",
              " 'mri': 0.99,\n",
              " 'mya': 14.676,\n",
              " 'nld': 26.502,\n",
              " 'nob': 33.007,\n",
              " 'npi': 23.518,\n",
              " 'nso': 1.955,\n",
              " 'nya': 16.36,\n",
              " 'oci': 17.988,\n",
              " 'ory': 21.637,\n",
              " 'pan': 23.095,\n",
              " 'pbt': 11.242,\n",
              " 'pes': 27.74,\n",
              " 'pol': 22.163,\n",
              " 'por': 38.599,\n",
              " 'ron': 32.337,\n",
              " 'rus': 27.826,\n",
              " 'slk': 31.104,\n",
              " 'slv': 23.925,\n",
              " 'sna': 2.648,\n",
              " 'snd': 6.712,\n",
              " 'som': 14.259,\n",
              " 'spa': 24.544,\n",
              " 'srp': 35.158,\n",
              " 'swe': 33.979,\n",
              " 'swh': 25.754,\n",
              " 'tam': 15.136,\n",
              " 'tel': 20.319,\n",
              " 'tgk': 25.265,\n",
              " 'tgl': 21.896,\n",
              " 'tha': 18.013,\n",
              " 'tur': 24.819,\n",
              " 'ukr': 29.938,\n",
              " 'umb': 0.363,\n",
              " 'urd': 22.226,\n",
              " 'uzn': 21.475,\n",
              " 'vie': 20.481,\n",
              " 'wol': 0.832,\n",
              " 'xho': 3.126,\n",
              " 'yor': 12.487,\n",
              " 'yue': 12.381,\n",
              " 'zlm': 29.128,\n",
              " 'zul': 5.229}"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('seamless_large_fleurs_bleu.json','w')as f:\n",
        "  json.dump(seamless_fleurs_bleu,f)"
      ],
      "metadata": {
        "id": "0i9Eat4-ldAe"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print(datasets.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at26E_5HqU2p",
        "outputId": "82cac807-ffeb-4a75-a78c-e763ec184d4e"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJGFh4NA5lAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}