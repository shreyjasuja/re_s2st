{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcjLYl0FU1pO"
      },
      "source": [
        "## Evaluation on FLEURS dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyjasuja/re_s2st/blob/main/fleurs_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we evaluate the performance of various multilingual multitask models on the FLEURS dataset. Fleurs is the speech version of the FLoRes machine translation benchmark. They use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. The dataset is available at [Huggingface datasets](https://huggingface.co/datasets/google/fleurs). \n",
        "\n",
        "We evaluate the performance of the following models on the FLEURS dataset:\n",
        "\n",
        "*   [Whisper](https://arxiv.org/pdf/2212.04356.pdf) (Radford et al., 2022)\n",
        "\n",
        "*   [SeamlessM4T](https://arxiv.org/pdf/2308.11596.pdf) (Barrault et al., 2023)\n",
        "\n",
        "*   Cascaded pipeline of Whisper(ASR) and [NLLB-1.3B (MT)](https://arxiv.org/ftp/arxiv/papers/2207/2207.04672.pdf) (MR Costa-juss√† et al., 2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The important thing to note here is that not all languages in the FLEURS dataset are supported by the models. Whisper supports only 82 languages (including English), while Seamless and NLLB supports all the languages in the dataset. So, in order to evaluate the performance of Whisper on the FLEURS dataset, we will only consider the 81 languages supported by Whisper, but during inference we considered all the supported languages for a given model.\n",
        "\n",
        "You will also find the reference of AudioPaLM [Rubenstein et al., 2023](https://arxiv.org/pdf/2306.12925.pdf) model in the notebook which is yet another model that supports all the languages in the FLEURS dataset. However, we are not evaluating the performance of AudioPaLM in this notebook, because the model is a proprietary model and the weights are not available for download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQBQ4i1iMywL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Language code mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the FLEURS dataset from Huggingface ü§ó datasets. Interestingly, we need to use BCP-47 codes in order to access various languages in the dataset, and Seamless paper uses ISO 639-3 language code as their standard, so we had to work on a mapping between the two. We used `pycountry` library to resolve the mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OHIYGM2yjNOr"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_config_names\n",
        "bcp_47_codes=get_dataset_config_names(\"google/fleurs\",trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G66B1hRoU1QI"
      },
      "outputs": [],
      "source": [
        "from pycountry import languages\n",
        "def bcp47_to_iso639_3(bcp47_code):\n",
        "    parts = bcp47_code.split('_')\n",
        "    lang_code = parts[0]\n",
        "    try:\n",
        "\n",
        "      lang = languages.get(alpha_2=lang_code).alpha_3\n",
        "      return lang.lower()\n",
        "    except (AttributeError, KeyError) as e:\n",
        "        # If the mapping fails, return the original code\n",
        "        print(\"Failed to map\", lang_code, \" from bcp47 to iso639-3\")\n",
        "        return lang_code.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LFdNikSU9UL",
        "outputId": "b28529a4-c5b6-4da0-97c4-7260015ff42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to map ast  from bcp47 to iso639-3\n",
            "Failed to map ceb  from bcp47 to iso639-3\n",
            "Failed to map ckb  from bcp47 to iso639-3\n",
            "Failed to map cmn  from bcp47 to iso639-3\n",
            "Failed to map fil  from bcp47 to iso639-3\n",
            "Failed to map kam  from bcp47 to iso639-3\n",
            "Failed to map kea  from bcp47 to iso639-3\n",
            "Failed to map luo  from bcp47 to iso639-3\n",
            "Failed to map nso  from bcp47 to iso639-3\n",
            "Failed to map umb  from bcp47 to iso639-3\n",
            "Failed to map yue  from bcp47 to iso639-3\n",
            "Failed to map all  from bcp47 to iso639-3\n"
          ]
        }
      ],
      "source": [
        "lang_dict={}\n",
        "for i in bcp_47_codes:\n",
        "  lang_dict[bcp47_to_iso639_3(i)]=i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seamless team provided us with the exact ids of the input speech utterances that they used for evaluation. We use these ids to evaluate the performance of theri own model, Whisper and NLLB on the FLEURS dataset. This link was found on their ü§ó [model card](https://huggingface.co/facebook/seamless-m4t-large) under the metrics section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXNZiPrUbZYi",
        "outputId": "59be04c9-a0b3-4fd7-b205-d8a8a7e3660f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-17 15:16:11--  https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.34, 3.162.163.19, 3.162.163.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.34|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 OK\n",
            "Length: 6096377 (5.8M) [application/zip]\n",
            "Saving to: ‚Äòevaluation_data_ids.zip‚Äô\n",
            "\n",
            "evaluation_data_ids 100%[===================>]   5.81M  2.50MB/s    in 2.3s    \n",
            "\n",
            "2024-04-17 15:16:14 (2.50 MB/s) - ‚Äòevaluation_data_ids.zip‚Äô saved [6096377/6096377]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip -O evaluation_data_ids.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAe8UrAHneNd",
        "outputId": "34301e4b-21eb-4ade-9a97-0f155294b9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  evaluation_data_ids.zip\n",
            "replace evaluation_data_ids/s2st_cvss_ids/test_cvss_jpn-eng.ids? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!unzip evaluation_data_ids.zip && rm evaluation_data_ids.zip #unzip the evaluation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‚õîÔ∏è Caution:** We noticed that the ISO 639-3 code which we got from pycountry library is not always the same as the one used in the Seamless paper. So, we had to manually map some of the languages to the correct ISO 639-3 code. We have provided the mapping in the code below.\n",
        "\n",
        "There was one more bizzare thing issue we noticed in seamless paper that although they cited the ISO 639-3 code for the languages supported by their model in Table 5 in the paper, they still used a different code in the evaluation ids. We had to manually map these codes as well. example here, would be Norwegian which is `nob` and `nno` in the paper but `nor` in the evaluation ids. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "p2I1pSjmqi7_"
      },
      "outputs": [],
      "source": [
        "old_codes=['msa','fil','uzb','fas','nep','lav','ara','aze','pus','ori','mon','swa','orm']\n",
        "new_codes=['zlm','tgl','uzn','pes','npi','lvs','arb','azj','pbt','ory','khk','swh','gaz']\n",
        "for i in range(len(new_codes)):\n",
        "  lang_dict[new_codes[i]]=lang_dict[old_codes[i]]\n",
        "  del lang_dict[old_codes[i]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Restricting only to X‚ÜíEn directions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686SAdR3shmC",
        "outputId": "acd476e6-b6dd-41a4-92db-8f4c1c17c51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_path=\"evaluation_data_ids/s2tt_fleurs_ids/\"\n",
        "x_eng_files = [file for file in os.listdir(\"evaluation_data_ids/s2tt_fleurs_ids/\") if file.endswith('-eng.ids')]\n",
        "print(len(x_eng_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A sneak peek into the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**‚ùóÔ∏è Note:** We are using the FLEURS dataset in streaming model, as we are just evaluating the performance of the models on the dataset on the test split. As the dataset is huge, we also tried downloading just the test split for all languages at once using the `split` parameter and also with `data_files` parameter, as mentioned in the [documentation](https://huggingface.co/docs/datasets/loading), but it didn't work as it was still downloading the whole dataset.\n",
        "\n",
        "So, we ultimately resorted to the streaming mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZYl81tzoHjK",
        "outputId": "868eac9c-8c91-484a-b631-5cb9224da32d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict['ukr'],split=\"test\",streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the sample from the dataset, the id which we got from evaluation benchmark is the corresponding filename in the `path` under `audio` field. Please don't confuse it with the actual id in each record. \n",
        "\n",
        "```python \n",
        "{\n",
        "    'id': 1982, \n",
        "    'num_samples': 118080, \n",
        "    'path': None, \n",
        "    'audio': {\n",
        "        'path': 'test/10021730821550109934.wav', \n",
        "        'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
        "       -1.80006027e-05, -2.40206718e-05, -9.59634781e-05]), \n",
        "       'sampling_rate': 16000}, \n",
        "    'transcription': '–∂—ñ–Ω–∫–∏ —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏ —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É', 'raw_transcription': '–ñ—ñ–Ω–∫–∏: —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏, —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ, –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É.', \n",
        "    'gender': 0, \n",
        "    'lang_id': 92, \n",
        "    'language': 'Ukrainian', \n",
        "    'lang_group_id': 1\n",
        "    }\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XtWrSCg1Cj0",
        "outputId": "ee139ab8-4d8a-4984-f7ac-76ad67fbffac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1982, 'num_samples': 118080, 'path': None, 'audio': {'path': 'test/10021730821550109934.wav', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.80006027e-05, -2.40206718e-05, -9.59634781e-05]), 'sampling_rate': 16000}, 'transcription': '–∂—ñ–Ω–∫–∏ —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏ —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É', 'raw_transcription': '–ñ—ñ–Ω–∫–∏: —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏, —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ, –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É.', 'gender': 0, 'lang_id': 92, 'language': 'Ukrainian', 'lang_group_id': 1}\n"
          ]
        }
      ],
      "source": [
        "print(next(iter(src_lang_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You would notice that there would be multiple utterances for the same id in the dataset, as the dataset is a parallel corpus. Also, within the same language there are multiple speakers of different genders.\n",
        "\n",
        "So, when we would require reference ground truth for the evaluation, we would have to map on the actual id (and not utterances),and then take the english translation in `raw_transcription` field as the reference ground truth. You will see more on this in the next section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation metrics and code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We would be using `BLEU` score as our standard metric to evaluate the performance of the models on the FLEURS dataset. As mentioned in the paper in Table 4, we would be using the `sacrebleu` library to calculate the BLEU score.\n",
        "\n",
        "As we mentioned earlier, we would be using the English language in FLEURS dataset as the reference ground truth for the evaluation. We would be using the `raw_transcription` field in the dataset as the reference ground truth for the evaluation. \n",
        "\n",
        "**Note:** Although we used only test split for each individual language in the dataset, for english we would need to get entire dataset instead to get all the ids as these ids are randomly split under different splits across the different languages in the dataset. When we would be doing inference on the models, we would save the generated translations as a hash_map with `filenameId_actualId. This would preserve the generated translations for each unique utterance for a given actual id and the actual id is then used here to fetch the reference ground truth from the English data. for example: \n",
        "\n",
        "```python \n",
        "{\n",
        "    'id': 1982, \n",
        "    'num_samples': 118080, \n",
        "    'path': None, \n",
        "    'audio': {\n",
        "        'path': 'test/10021730821550109934.wav', \n",
        "        'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
        "       -1.80006027e-05, -2.40206718e-05, -9.59634781e-05]), \n",
        "       'sampling_rate': 16000}, \n",
        "    'transcription': '–∂—ñ–Ω–∫–∏ —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏ —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É', 'raw_transcription': '–ñ—ñ–Ω–∫–∏: —É—Å—ñ–º –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º –∂—ñ–Ω–∫–∞–º —Ä–∞–¥—è—Ç—å –∫–∞–∑–∞—Ç–∏, —â–æ –≤–æ–Ω–∏ –∑–∞–º—ñ–∂–Ω—ñ, –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Å—ñ–º–µ–π–Ω–æ–≥–æ —Å—Ç–∞–Ω—É.', \n",
        "    'gender': 0, \n",
        "    'lang_id': 92, \n",
        "    'language': 'Ukrainian', \n",
        "    'lang_group_id': 1\n",
        "    }\n",
        "\n",
        "```\n",
        "\n",
        "For this we made the key as `10021730821550109934_1982` and the value as the generated translation. During evaluation, we would use the actual id `1982` to fetch the reference ground truth from the English data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H40gXeodK-5F"
      },
      "outputs": [],
      "source": [
        "def get_bleu_score(generated_translations):\n",
        "\n",
        "  # get english data from fleurs dataset\n",
        "  eng_data=load_dataset('google/fleurs',name='en_us',trust_remote_code=True)\n",
        "  eng_translation={}\n",
        "  #combine all english translations from all splits\n",
        "  for split in eng_data: \n",
        "    for item in tqdm(eng_data[split]):\n",
        "      audio_sample = item['audio']\n",
        "      eng_translation[item['id']]=item['raw_transcription']\n",
        "\n",
        "  bleu_score={}\n",
        "  for lang_code in list(generated_translations.keys()): #calculate bleu score for each language code\n",
        "\n",
        "    translations=[]\n",
        "    gt_translations=[]\n",
        "\n",
        "    for i in generated_translations[lang_code]:\n",
        "        key=int(i.split('_')[0]) #actual id \n",
        "        gt_translations.append(eng_translation[key])\n",
        "        translations.append(generated_translations[lang_code][i])\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(translations, [gt_translations])\n",
        "    bleu_score[lang_code]=round(bleu.score, 3)\n",
        "\n",
        "  return bleu_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setting up the results directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "results_directory='results/fleurs'\n",
        "if not os.path.exists(results_directory):\n",
        "  os.makedirs(os.path.join(results_directory,'scores'))\n",
        "  os.makedirs(os.path.join(results_directory,'generations'))\n",
        "\n",
        "scores_path=os.path.join(results_directory,'scores')\n",
        "generations_path=os.path.join(results_directory,'generations')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Seamless models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The claims under our study are evaluated on both Seamless medium and large models. Both models differ only in number of parameters, thus overall inference methods remains the same.\n",
        "\n",
        " ‚ùó **Note** : *In order to evaluate the performance of seamless models on CoVoST2 data, just change the `model_type` according to medium or large models, and run the code under this section.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_type=\"medium\"\n",
        "model_type=\"large\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We would be using Seamless models added to HuggingFace ü§ó by Facebook, you can find more information about this from the [model card](https://huggingface.co/facebook/seamless-m4t-medium) The code in this section has been adopted from documentation available [here](https://huggingface.co/docs/transformers/v4.38.0/en/model_doc/seamless_m4t#overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzSLzRZ3pZpg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-\"+model_type)\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-\"+model_type)\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inferencing on this large dataset is going take a lot of time so we need to be patient. For us, on a single RTX 6000 GPU, it took around 2 days to generate translations for the entire test split of the FLEURS dataset for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rFOtLKC1nhfw"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "generated_translations=collections.defaultdict(dict) #hashmap to store translations for each language code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fWWb2q8vPCms"
      },
      "outputs": [],
      "source": [
        "lang_issue=[]\n",
        "lang_missing_ids=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the inference time is large, we made the code in such a way that during any interruption, the code would save the generated translations in a file, so that we can resume the inference from where we left off. It eliminates any redundancy in the inference process. \n",
        "\n",
        "We would be doing inference only on the audio utterances mentiones in the evaluation benchmark provided by Seamless team. We would be saving the generated translations in a file with the key as `filenameId_actualId` and the value as the generated translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p6PX1kn0xBa",
        "outputId": "020e7e09-9ee3-4938-ee6d-c2cda0d4b139"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [09:11<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 908/908 [11:45<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 926/926 [12:11<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 728/728 [08:41<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 749/749 [08:40<00:00,  1.44it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1015/1015 [12:48<00:00,  1.32it/s]\n",
            "264it [03:08,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1041/1041 [15:38<00:00,  1.11it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 918/918 [11:43<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 357/357 [04:26<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1021/1021 [13:00<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [11:24<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946/946 [11:01<00:00,  1.43it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 977/977 [11:28<00:00,  1.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 687/687 [08:17<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 857/857 [10:27<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 660/660 [11:18<00:00,  1.03s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 621/621 [09:49<00:00,  1.05it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792/792 [09:20<00:00,  1.41it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 925/925 [15:11<00:00,  1.01it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1008/1008 [17:31<00:00,  1.04s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 893/893 [11:12<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:18<00:00,  1.35it/s]\n",
            " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 237/379 [03:57<02:31,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 3189556219205510204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 312/379 [05:56<00:58,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 6634898757415929965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 379/379 [08:08<00:00,  1.29s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 905/905 [11:21<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 964/964 [13:15<00:00,  1.21it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 919/919 [11:39<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 920/920 [11:09<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 831/831 [10:39<00:00,  1.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 771/771 [10:06<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 364/364 [04:10<00:00,  1.45it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 979/979 [12:03<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 854/854 [12:44<00:00,  1.12it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 980/980 [12:53<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1019/1019 [13:19<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [05:31<00:00,  1.12it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 723/723 [09:26<00:00,  1.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1021/1021 [13:15<00:00,  1.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 862/862 [10:54<00:00,  1.32it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [03:40<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 871/871 [11:20<00:00,  1.28it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [09:15<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [11:28<00:00,  1.45it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 838/838 [10:20<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 969/969 [12:56<00:00,  1.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 574/574 [06:31<00:00,  1.47it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 726/726 [08:38<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 883/883 [10:43<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792/792 [09:56<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 856/856 [11:18<00:00,  1.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 862/862 [11:09<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 958/958 [11:56<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 851/851 [10:53<00:00,  1.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 923/923 [12:15<00:00,  1.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 922/922 [10:48<00:00,  1.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 428/428 [05:02<00:00,  1.41it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 834/834 [10:17<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [07:56<00:00,  1.26it/s]\n",
            " 14%|‚ñà‚ñé        | 55/405 [00:42<04:34,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 12560373056138365189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 405/405 [07:16<00:00,  1.08s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 984/984 [11:58<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 761/761 [10:01<00:00,  1.26it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 759/759 [09:22<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512/512 [06:43<00:00,  1.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 998/998 [14:30<00:00,  1.15it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [08:00<00:00,  1.00s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 865/865 [11:54<00:00,  1.21it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 973/973 [12:13<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 382/382 [04:34<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 723/723 [08:57<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 932/932 [11:28<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 927/927 [11:11<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [08:19<00:00,  1.35it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 883/883 [10:34<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 827/827 [13:04<00:00,  1.05it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 949/949 [11:16<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:39<00:00,  1.16it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541/541 [07:16<00:00,  1.24it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [11:13<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 775/775 [09:55<00:00,  1.30it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 819/819 [10:04<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [04:19<00:00,  1.01s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 487/487 [06:12<00:00,  1.31it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 758/758 [08:53<00:00,  1.42it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 940/940 [11:49<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 986/986 [12:15<00:00,  1.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 930/930 [11:23<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:36<00:00,  1.11it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 934/934 [11:19<00:00,  1.37it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 418/418 [04:59<00:00,  1.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 472/472 [05:40<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 700/700 [08:36<00:00,  1.36it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 864/864 [10:51<00:00,  1.33it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 967/967 [13:48<00:00,  1.17it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 658/658 [07:56<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [12:09<00:00,  1.08it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 650/650 [08:22<00:00,  1.29it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 650/650 [07:48<00:00,  1.39it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 945/945 [11:24<00:00,  1.38it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 925/925 [12:02<00:00,  1.28it/s]\n",
            " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 554/842 [07:26<03:11,  1.51it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "for file_name in  x_eng_files:\n",
        "  lang_code = file_name.split(\"-\")[0].split(\"_\")[-1]\n",
        "  with open(base_path+file_name) as f:\n",
        "    ids=f.read().split()\n",
        "\n",
        "  if (lang_code in generated_translations.keys()) and (len(ids)==len(generated_translations[lang_code])) :\n",
        "    print(\"Done\")\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict[lang_code],split=\"test\",streaming=True,trust_remote_code=True)\n",
        "  except:\n",
        "    lang_issue.append(lang_code)\n",
        "    print(\"\\n Missing language \",lang_code)\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "  for item in tqdm(src_lang_data,total=len(ids)):\n",
        "    audio_sample = item['audio']\n",
        "    id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    if str(item['id'])+'_'+str(id) in generated_translations[lang_code]:\n",
        "      continue\n",
        "\n",
        "    if id not in ids:\n",
        "      continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Initially, try to process the audio on the GPU\n",
        "        audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\", sampling_rate=16000)\n",
        "        audio_inputs = {k: v.to('cuda') for k, v in audio_inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "        translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"\\nCUDA out of memory. Shifting inference to CPU for ID:\", id)\n",
        "            torch.cuda.empty_cache()  # Clear any unreleased memory\n",
        "\n",
        "            # Move the model to CPU for this inference\n",
        "            model.to('cpu')\n",
        "\n",
        "            try:\n",
        "                # Make sure audio_inputs are on the CPU as well\n",
        "                audio_inputs = {k: v.to('cpu') for k, v in audio_inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "                translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "            except Exception as cpu_e:\n",
        "                print(\"\\nFailed processing on CPU for ID:\", id, \"with error:\", cpu_e)\n",
        "                lang_missing_ids.append((lang_code, id))\n",
        "            finally:\n",
        "                # Regardless of the outcome, put the model back on the GPU for subsequent operations\n",
        "                model.to('cuda')\n",
        "        else:\n",
        "            print(\"\\nAn error occurred for ID:\", id, \"Error:\", e)\n",
        "            lang_missing_ids.append((lang_code, id))\n",
        "    except Exception as e:\n",
        "        print(\"\\nAn unexpected error occurred for ID:\", id, \"Error:\", e)\n",
        "        lang_missing_ids.append((lang_code, id))\n",
        "\n",
        "\n",
        "    del audio_inputs\n",
        "    del output_tokens\n",
        "\n",
        "    generated_translations[lang_code][str(item['id'])+'_'+str(id)]=translation\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  with open (os.path.join(generations_path,'Seamless '+model_type+'.json'),'w')as f:\n",
        "    json.dump(generated_translations,f,indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYrtUcIzFosB",
        "outputId": "81dfe5dd-e074-467c-d39b-514cd41163cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Dr8J1tTJWr-a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "with open(os.path.join(generations_path,'Seamless '+model_type+'.json')) as f:\n",
        "    generated_translations = json.load(f)\n",
        "\n",
        "# Convert to defaultdict with empty dictionaries as default values\n",
        "generated_translations = defaultdict(dict, generated_translations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets evaluate against the reference English transcriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQkxs_E8NKDs",
        "outputId": "54afd1c4-21da-4654-96cb-973f204183a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:02<00:00, 921.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:00<00:00, 989.93it/s] \n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 647/647 [00:00<00:00, 949.56it/s]\n"
          ]
        }
      ],
      "source": [
        "seamless_fleurs_bleu=get_bleu_score(generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q-0FxJrXPv-",
        "outputId": "74ebb769-d2c8-4e92-db70-56765052e551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'afr': 39.69,\n",
              " 'amh': 17.034,\n",
              " 'arb': 31.725,\n",
              " 'asm': 17.47,\n",
              " 'ast': 25.894,\n",
              " 'azj': 16.425,\n",
              " 'bel': 16.056,\n",
              " 'ben': 22.778,\n",
              " 'bos': 32.891,\n",
              " 'bul': 31.33,\n",
              " 'cat': 37.574,\n",
              " 'ceb': 7.723,\n",
              " 'ces': 31.016,\n",
              " 'ckb': 20.487,\n",
              " 'cmn': 18.98,\n",
              " 'cym': 30.22,\n",
              " 'dan': 33.553,\n",
              " 'deu': 35.469,\n",
              " 'ell': 24.804,\n",
              " 'est': 28.534,\n",
              " 'fin': 25.782,\n",
              " 'fra': 32.641,\n",
              " 'ful': 0.788,\n",
              " 'gaz': 0.317,\n",
              " 'gle': 10.654,\n",
              " 'glg': 32.033,\n",
              " 'guj': 27.164,\n",
              " 'hau': 0.544,\n",
              " 'heb': 28.226,\n",
              " 'hin': 25.194,\n",
              " 'hrv': 29.8,\n",
              " 'hun': 24.166,\n",
              " 'hye': 27.81,\n",
              " 'ibo': 1.27,\n",
              " 'ind': 28.81,\n",
              " 'isl': 22.854,\n",
              " 'ita': 25.307,\n",
              " 'jav': 19.459,\n",
              " 'jpn': 15.886,\n",
              " 'kam': 1.803,\n",
              " 'kan': 21.799,\n",
              " 'kat': 18.741,\n",
              " 'kaz': 21.338,\n",
              " 'kea': 27.313,\n",
              " 'khk': 16.258,\n",
              " 'khm': 18.62,\n",
              " 'kir': 16.771,\n",
              " 'kor': 18.402,\n",
              " 'lao': 19.088,\n",
              " 'lin': 0.917,\n",
              " 'lit': 20.675,\n",
              " 'ltz': 14.429,\n",
              " 'lug': 16.179,\n",
              " 'luo': 0.789,\n",
              " 'lvs': 27.666,\n",
              " 'mal': 20.99,\n",
              " 'mar': 21.372,\n",
              " 'mkd': 33.972,\n",
              " 'mlt': 38.23,\n",
              " 'mri': 0.99,\n",
              " 'mya': 14.676,\n",
              " 'nld': 26.502,\n",
              " 'nob': 33.007,\n",
              " 'npi': 23.518,\n",
              " 'nso': 1.955,\n",
              " 'nya': 16.36,\n",
              " 'oci': 17.988,\n",
              " 'ory': 21.637,\n",
              " 'pan': 23.095,\n",
              " 'pbt': 11.242,\n",
              " 'pes': 27.74,\n",
              " 'pol': 22.163,\n",
              " 'por': 38.599,\n",
              " 'ron': 32.337,\n",
              " 'rus': 27.826,\n",
              " 'slk': 31.104,\n",
              " 'slv': 23.925,\n",
              " 'sna': 2.648,\n",
              " 'snd': 6.712,\n",
              " 'som': 14.259,\n",
              " 'spa': 24.544,\n",
              " 'srp': 35.158,\n",
              " 'swe': 33.979,\n",
              " 'swh': 25.754,\n",
              " 'tam': 15.136,\n",
              " 'tel': 20.319,\n",
              " 'tgk': 25.265,\n",
              " 'tgl': 21.896,\n",
              " 'tha': 18.013,\n",
              " 'tur': 24.819,\n",
              " 'ukr': 29.938,\n",
              " 'umb': 0.363,\n",
              " 'urd': 22.226,\n",
              " 'uzn': 21.475,\n",
              " 'vie': 20.481,\n",
              " 'wol': 0.832,\n",
              " 'xho': 3.126,\n",
              " 'yor': 12.487,\n",
              " 'yue': 12.381,\n",
              " 'zlm': 29.128,\n",
              " 'zul': 5.229}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(sorted(seamless_fleurs_bleu.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0i9Eat4-ldAe"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(scores_path,'Seamless '+model_type+'.json'),'w')as f:\n",
        "  json.dump(seamless_fleurs_bleu,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Whisper model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59MfpGGyR464",
        "outputId": "af3d2620-dfcc-4538-fc62-e672522a5f83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
        "model.to('cuda')\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filter and map language codes for Whisper model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We found that the Whisper model doesn't support the ISO 639-3 language codes, instead it either takes the 2 letter language codes or the full language name or we can omit giving any language code but that degrades the performance as the model then would also need to perform Language Identification task, and error due to language identification would be added to the overall error. So, we again need to map the ISO 639-3 language codes to the language codes supported by the Whisper model. \n",
        "\n",
        "We used the `pycountry` library to get the 2 letter language codes for the ISO 639-3 language codes. As we mentioned in the beginning that the Whisper model doesn't support all the languages in the FLEURS dataset, so we would only consider the languages supported by the Whisper model for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wJL2avl8cWSs"
      },
      "outputs": [],
      "source": [
        "allowed_whisper_lang=[\n",
        "            'en', 'zh', 'de', 'es', 'ru', 'ko', 'fr', 'ja', 'pt', 'tr', 'pl', 'ca', 'nl', 'ar', 'sv', 'it', 'id', 'hi', 'fi', 'vi',\n",
        "            'he', 'uk', 'el', 'ms', 'cs', 'ro', 'da', 'hu', 'ta', 'no', 'th', 'ur', 'hr', 'bg', 'lt', 'la', 'mi', 'ml', 'cy', 'sk', \n",
        "            'te', 'fa', 'lv', 'bn', 'sr', 'az', 'sl', 'kn', 'et', 'mk', 'br', 'eu', 'is', 'hy', 'ne', 'mn', 'bs', 'kk', 'sq', 'sw', \n",
        "            'gl', 'mr', 'pa', 'si', 'km', 'sn', 'yo', 'so', 'af', 'oc', 'ka', 'be', 'tg', 'sd', 'gu', 'am', 'yi', 'lo', 'uz', 'fo',\n",
        "            'ht', 'ps', 'tk', 'nn', 'mt', 'sa', 'lb', 'my', 'bo', 'tl', 'mg', 'as', 'tt', 'haw', 'ln', 'ha', 'ba', 'jw', 'su', \n",
        "            'yue', 'my', 'ca', 'nl', 'ht', 'lb', 'ps', 'pa', 'ro', 'ro', 'si', 'es', 'zh']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TrXHi9b3-xM_"
      },
      "outputs": [],
      "source": [
        "import pycountry\n",
        "\n",
        "def iso639_3_to_iso639_1(code_639_3):\n",
        "    # remove older seamless specific mappings\n",
        "    cross_mapping = dict(zip(new_codes, old_codes))\n",
        "\n",
        "    # Additional special cases for direct conversion from ISO 639-3 to ISO 639-1\n",
        "    special_cases = {\n",
        "        'cmn': 'zh',  # Mandarin Chinese\n",
        "        'nob': 'no',  # Norwegian Bokm√•l\n",
        "        'jav': 'jw'   # Javanese\n",
        "        # Add any other special cases if needed\n",
        "    }\n",
        "\n",
        "    # Check special cases first\n",
        "    if code_639_3 in special_cases:\n",
        "        return special_cases[code_639_3]\n",
        "\n",
        "    # Use cross-mapping to find the corresponding old code if available\n",
        "    if code_639_3 in cross_mapping:\n",
        "        code_639_3 = cross_mapping[code_639_3]\n",
        "\n",
        "    # Use pycountry to attempt to convert any code to ISO 639-1 two-letter code\n",
        "    try:\n",
        "        language = pycountry.languages.get(alpha_3=code_639_3)\n",
        "        return language.alpha_2 if hasattr(language, 'alpha_2') else code_639_3\n",
        "    except AttributeError:\n",
        "        # Return the original code if no ISO 639-1 code is found\n",
        "        return code_639_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vyrMWWDW_o2d"
      },
      "outputs": [],
      "source": [
        "whisper_codes={}\n",
        "for code in lang_dict:\n",
        "  x=iso639_3_to_iso639_1(code)\n",
        "  if x in allowed_whisper_lang:\n",
        "    whisper_codes[code]=x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U8ksMLY_vDW",
        "outputId": "bfe66fb8-0608-4ea7-93c9-f99f37ec64ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(whisper_codes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "EhPGic3Ngd0k"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "generated_transcriptions=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IAoXwEn4TpOL"
      },
      "outputs": [],
      "source": [
        "lang_issue=[]\n",
        "lang_missing_ids=[]\n",
        "options = dict(beam_size=5, best_of=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inference code is similar to that of Seamless model, but the only difference is that we are using the 2 letter language codes for the Whisper model when passing to its decoder.\n",
        "\n",
        "**‚ùóÔ∏è Note:** Although we talk about 2-letter language code, but while saving the generated translations, we would be still saving the key as `filenameId_actualId` and the value as the generated translation under the ISO 639-3 language code as nested dictionary. This would allow for easier comparison during final analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the inference while decoding we did one extra step to use the model as ASR and generate transcriptions. This would come handy when we use the cascaded pipeline where whisper had to perform as ASR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJGFh4NA5lAg",
        "outputId": "753e2e68-4a02-40ae-bbe0-0592c09ed7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 958/958 [1:08:55<00:00,  4.32s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 932/932 [1:01:08<00:00,  3.94s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 883/883 [27:59<00:00,  1.90s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 934/934 [31:05<00:00,  2.00s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792/792 [26:30<00:00,  2.01s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 775/775 [23:39<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper ckb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 834/834 [26:26<00:00,  1.90s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 905/905 [30:33<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper kir\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [21:41<00:00,  1.75s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 726/726 [41:45<00:00,  3.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper wol\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 945/945 [27:56<00:00,  1.77s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1015/1015 [58:49<00:00,  3.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper ful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|‚ñä         | 67/851 [02:24<30:03,  2.30s/it]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "for file_name in  x_eng_files:\n",
        "  lang_code = file_name.split(\"-\")[0].split(\"_\")[-1]\n",
        "  with open(base_path+file_name) as f:\n",
        "    ids=f.read().split()\n",
        "\n",
        "  if (lang_code in generated_translations.keys()) and (len(ids)==len(generated_translations[lang_code])) :\n",
        "    print(\"Done\")\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "\n",
        "    forced_decoder_ids={\n",
        "    'translate' : processor.get_decoder_prompt_ids(language=whisper_codes[lang_code], task=\"translate\"),\n",
        "    'transcribe': processor.get_decoder_prompt_ids(language=whisper_codes[lang_code], task=\"transcribe\")\n",
        "    }\n",
        "    src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict[lang_code],split=\"test\",streaming=True,trust_remote_code=True)\n",
        "\n",
        "  except:\n",
        "    lang_issue.append(lang_code)\n",
        "    print(\"\\n Missing language in whisper\",lang_code)\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for item in tqdm(src_lang_data,total=len(ids)):\n",
        "    audio_sample = item['audio']\n",
        "    id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    if str(item['id'])+'_'+str(id) in generated_translations[lang_code]:\n",
        "      continue\n",
        "\n",
        "    if id not in ids:\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "        # Initially, try to process the audio on the GPU\n",
        "        input_features = processor(audio_sample[\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "        input_features= input_features.to('cuda')\n",
        "        with torch.no_grad():\n",
        "          translate_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['translate'])\n",
        "          transcript_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['transcribe'])\n",
        "\n",
        "        translation = processor.batch_decode(translate_output, skip_special_tokens=True)\n",
        "        transcription = processor.batch_decode(transcript_output, skip_special_tokens=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"\\nCUDA out of memory. Shifting inference to CPU for ID:\", id)\n",
        "            torch.cuda.empty_cache()  # Clear any unreleased memory\n",
        "\n",
        "            # Move the model to CPU for this inference\n",
        "            model.to('cpu')\n",
        "\n",
        "            try:\n",
        "                input_features= input_features.to('cpu')\n",
        "                with torch.no_grad():\n",
        "                  translate_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['translate'])\n",
        "                  transcript_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['transcribe'])\n",
        "\n",
        "                translation = processor.batch_decode(translate_output, skip_special_tokens=True)\n",
        "                transcription = processor.batch_decode(transcript_output, skip_special_tokens=True)\n",
        "            except Exception as cpu_e:\n",
        "                print(\"\\nFailed processing on CPU for ID:\", id, \"with error:\", cpu_e)\n",
        "                lang_missing_ids.append((lang_code, id))\n",
        "            finally:\n",
        "                # Regardless of the outcome, put the model back on the GPU for subsequent operations\n",
        "                model.to('cuda')\n",
        "        else:\n",
        "            print(\"\\nAn error occurred for ID:\", id, \"Error:\", e)\n",
        "            lang_missing_ids.append((lang_code, id))\n",
        "    except Exception as e:\n",
        "        print(\"\\nAn unexpected error occurred for ID:\", id, \"Error:\", e)\n",
        "        lang_missing_ids.append((lang_code, id))\n",
        "\n",
        "    del input_features ,transcript_output, translate_output\n",
        "\n",
        "    generated_translations[lang_code][str(item['id'])+'_'+str(id)]=translation\n",
        "    generated_transcriptions[lang_code][str(item['id'])+'_'+str(id)]=transcription\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  with open (os.path.join(generations_path,'Whisper large-v2.json'),'w')as f:\n",
        "    json.dump(generated_translations,f,indent=2)\n",
        "\n",
        "  with open (os.path.join(generations_path,'Whisper large-v2_asr.json'),'w')as f:\n",
        "    json.dump(generated_transcriptions,f,indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9hATtvtxHY"
      },
      "outputs": [],
      "source": [
        "len(generated_translations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "whisper_bleu_score= get_bleu_score(generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict(sorted(whisper_bleu_score.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(scores_path,'Whisper large-v2.json'),'w')as f:\n",
        "    json.dump(whisper_bleu_score,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cascaded pipeline of Whisper and NLLB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the NLLB model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the Whisper generated transcriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(generations_path,'Whisper large-v2_asr.json')) as f:\n",
        "     generated_transcriptions = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cascaded_pipeline_bleu_score= get_bleu_score(generated_transcriptions)\n",
        "\n",
        "dict(sorted(cascaded_pipeline_bleu_score.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(scores_path,'Whisper Large-v2 ASR + NLLB-1.3B.json'),'w')as f:\n",
        "    json.dump(cascaded_pipeline_bleu_score,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challanges, we overcame üí™ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Mapping of language codes**: We faced a lot of issues in mapping the language codes from ISO 639-3 to the language codes supported by the models. We had to manually map some of the languages to the correct ISO 639-3 code. We also noticed that the language codes used in the evaluation ids were different from the ones used in the paper. We had to manually map these codes as well. This wasn't limited to just Seamless, for inference on Whisper models, it required a 2-letter code which also led to significant effort as not all mappings were available.\n",
        "\n",
        "2. **Limited literature around FLEURS data**: There was very limited literature available around the FLEURS dataset. We had to rely on the information provided by the Seamless team in their model card and the dataset card on Huggingface. It took some time to realise that there were multiple utterances available for same actual id and that n-way mapping was provided on actual ids which could be shuffled across different splits in different languages\n",
        "\n",
        "3. **Inference time**: The inference time for the models was very large. It took around 2 days to generate translations for the entire test split of the FLEURS dataset for each model. We had to make the code in such a way that during any interruption, the code would save the generated translations in a file, so that we can resume the inference from where we left off.\n",
        "\n",
        "4. **Language support**: Not all languages in the FLEURS dataset are supported by the models. Whisper supports only 82 languages (including English), while Seamless and NLLB supports all the languages in the dataset. So, in order to evaluate the performance of Whisper on the FLEURS dataset, we only considered the 81 languages supported by Whisper, but during inference we considered all the supported languages for a given model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPDwRAz7UWFOhO/04F9pdv+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
