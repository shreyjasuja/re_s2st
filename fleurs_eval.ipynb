{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcjLYl0FU1pO"
      },
      "source": [
        "## Evaluation on FLEURS dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyjasuja/re_s2st/blob/main/fleurs_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "HQBQ4i1iMywL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G66B1hRoU1QI"
      },
      "outputs": [],
      "source": [
        "from pycountry import languages\n",
        "def bcp47_to_iso639_3(bcp47_code):\n",
        "    parts = bcp47_code.split('_')\n",
        "    lang_code = parts[0]\n",
        "    try:\n",
        "\n",
        "      lang = languages.get(alpha_2=lang_code).alpha_3\n",
        "      return lang.lower()\n",
        "    except (AttributeError, KeyError) as e:\n",
        "        # If the mapping fails, return the original code\n",
        "        return lang_code.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OHIYGM2yjNOr"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_config_names\n",
        "bcp_47_codes=get_dataset_config_names(\"google/fleurs\",trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LFdNikSU9UL",
        "outputId": "b28529a4-c5b6-4da0-97c4-7260015ff42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'alpha_3' ast\n",
            "'NoneType' object has no attribute 'alpha_3' ceb\n",
            "'NoneType' object has no attribute 'alpha_3' ckb\n",
            "'NoneType' object has no attribute 'alpha_3' cmn\n",
            "'NoneType' object has no attribute 'alpha_3' fil\n",
            "'NoneType' object has no attribute 'alpha_3' kam\n",
            "'NoneType' object has no attribute 'alpha_3' kea\n",
            "'NoneType' object has no attribute 'alpha_3' luo\n",
            "'NoneType' object has no attribute 'alpha_3' nso\n",
            "'NoneType' object has no attribute 'alpha_3' umb\n",
            "'NoneType' object has no attribute 'alpha_3' yue\n",
            "'NoneType' object has no attribute 'alpha_3' all\n"
          ]
        }
      ],
      "source": [
        "lang_dict={}\n",
        "for i in bcp_47_codes:\n",
        "  lang_dict[bcp47_to_iso639_3(i)]=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXNZiPrUbZYi",
        "outputId": "59be04c9-a0b3-4fd7-b205-d8a8a7e3660f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-16 15:07:44--  https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip\r\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.51, 3.162.163.34, 3.162.163.19, ...\r\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6096377 (5.8M) [application/zip]\n",
            "Saving to: ‘evaluation_data_ids.zip’\n",
            "\n",
            "evaluation_data_ids 100%[===================>]   5.81M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-04-16 15:07:45 (73.7 MB/s) - ‘evaluation_data_ids.zip’ saved [6096377/6096377]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip -O evaluation_data_ids.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAe8UrAHneNd",
        "outputId": "34301e4b-21eb-4ade-9a97-0f155294b9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  evaluation_data_ids.zip\n",
            "replace evaluation_data_ids/s2st_cvss_ids/test_cvss_jpn-eng.ids? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!unzip evaluation_data_ids.zip && rm evaluation_data_ids.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p2I1pSjmqi7_"
      },
      "outputs": [],
      "source": [
        "old_codes=['msa','fil','uzb','fas','nep','lav','ara','aze','pus','ori','mon','swa','orm']\n",
        "new_codes=['zlm','tgl','uzn','pes','npi','lvs','arb','azj','pbt','ory','khk','swh','gaz']\n",
        "for i in range(len(new_codes)):\n",
        "  lang_dict[new_codes[i]]=lang_dict[old_codes[i]]\n",
        "  del lang_dict[old_codes[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686SAdR3shmC",
        "outputId": "acd476e6-b6dd-41a4-92db-8f4c1c17c51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_path=\"evaluation_data_ids/s2tt_fleurs_ids/\"\n",
        "x_eng_files = [file for file in os.listdir(\"evaluation_data_ids/s2tt_fleurs_ids/\") if file.endswith('-eng.ids')]\n",
        "print(len(x_eng_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzSLzRZ3pZpg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-large\")\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "rFOtLKC1nhfw"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "generated_translations=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZYl81tzoHjK",
        "outputId": "868eac9c-8c91-484a-b631-5cb9224da32d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict['ukr'],split=\"test\",streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XtWrSCg1Cj0",
        "outputId": "ee139ab8-4d8a-4984-f7ac-76ad67fbffac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1982, 'num_samples': 118080, 'path': None, 'audio': {'path': 'test/10021730821550109934.wav', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.80006027e-05, -2.40206718e-05, -9.59634781e-05]), 'sampling_rate': 16000}, 'transcription': 'жінки усім подорожнім жінкам радять казати що вони заміжні незалежно від справжнього сімейного стану', 'raw_transcription': 'Жінки: усім подорожнім жінкам радять казати, що вони заміжні, незалежно від справжнього сімейного стану.', 'gender': 0, 'lang_id': 92, 'language': 'Ukrainian', 'lang_group_id': 1}\n"
          ]
        }
      ],
      "source": [
        "print(next(iter(src_lang_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fWWb2q8vPCms"
      },
      "outputs": [],
      "source": [
        "lang_issue=[]\n",
        "lang_missing_ids=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p6PX1kn0xBa",
        "outputId": "020e7e09-9ee3-4938-ee6d-c2cda0d4b139"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 750/750 [09:11<00:00,  1.36it/s]\n",
            "100%|██████████| 908/908 [11:45<00:00,  1.29it/s]\n",
            "100%|██████████| 926/926 [12:11<00:00,  1.27it/s]\n",
            "100%|██████████| 728/728 [08:41<00:00,  1.40it/s]\n",
            "100%|██████████| 749/749 [08:40<00:00,  1.44it/s]\n",
            "100%|██████████| 1015/1015 [12:48<00:00,  1.32it/s]\n",
            "264it [03:08,  1.40it/s]\n",
            "100%|██████████| 1041/1041 [15:38<00:00,  1.11it/s]\n",
            "100%|██████████| 918/918 [11:43<00:00,  1.31it/s]\n",
            "100%|██████████| 357/357 [04:26<00:00,  1.34it/s]\n",
            "100%|██████████| 1021/1021 [13:00<00:00,  1.31it/s]\n",
            "100%|██████████| 880/880 [11:24<00:00,  1.29it/s]\n",
            "100%|██████████| 946/946 [11:01<00:00,  1.43it/s]\n",
            "100%|██████████| 977/977 [11:28<00:00,  1.42it/s]\n",
            "100%|██████████| 687/687 [08:17<00:00,  1.38it/s]\n",
            "100%|██████████| 857/857 [10:27<00:00,  1.37it/s]\n",
            "100%|██████████| 660/660 [11:18<00:00,  1.03s/it]\n",
            "100%|██████████| 621/621 [09:49<00:00,  1.05it/s]\n",
            "100%|██████████| 792/792 [09:20<00:00,  1.41it/s]\n",
            "100%|██████████| 925/925 [15:11<00:00,  1.01it/s]\n",
            "100%|██████████| 1008/1008 [17:31<00:00,  1.04s/it]\n",
            "100%|██████████| 893/893 [11:12<00:00,  1.33it/s]\n",
            "100%|██████████| 591/591 [07:18<00:00,  1.35it/s]\n",
            " 63%|██████▎   | 237/379 [03:57<02:31,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 3189556219205510204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 312/379 [05:56<00:58,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 6634898757415929965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 379/379 [08:08<00:00,  1.29s/it]\n",
            "100%|██████████| 905/905 [11:21<00:00,  1.33it/s]\n",
            "100%|██████████| 964/964 [13:15<00:00,  1.21it/s]\n",
            "100%|██████████| 919/919 [11:39<00:00,  1.31it/s]\n",
            "100%|██████████| 920/920 [11:09<00:00,  1.38it/s]\n",
            "100%|██████████| 831/831 [10:39<00:00,  1.30it/s]\n",
            "100%|██████████| 771/771 [10:06<00:00,  1.27it/s]\n",
            "100%|██████████| 364/364 [04:10<00:00,  1.45it/s]\n",
            "100%|██████████| 979/979 [12:03<00:00,  1.35it/s]\n",
            "100%|██████████| 854/854 [12:44<00:00,  1.12it/s]\n",
            "100%|██████████| 980/980 [12:53<00:00,  1.27it/s]\n",
            "100%|██████████| 1019/1019 [13:19<00:00,  1.27it/s]\n",
            "100%|██████████| 371/371 [05:31<00:00,  1.12it/s]\n",
            "100%|██████████| 723/723 [09:26<00:00,  1.28it/s]\n",
            "100%|██████████| 1021/1021 [13:15<00:00,  1.28it/s]\n",
            "100%|██████████| 862/862 [10:54<00:00,  1.32it/s]\n",
            "100%|██████████| 299/299 [03:40<00:00,  1.36it/s]\n",
            "100%|██████████| 871/871 [11:20<00:00,  1.28it/s]\n",
            "100%|██████████| 743/743 [09:15<00:00,  1.34it/s]\n",
            "100%|██████████| 1000/1000 [11:28<00:00,  1.45it/s]\n",
            "100%|██████████| 838/838 [10:20<00:00,  1.35it/s]\n",
            "100%|██████████| 969/969 [12:56<00:00,  1.25it/s]\n",
            "100%|██████████| 574/574 [06:31<00:00,  1.47it/s]\n",
            "100%|██████████| 726/726 [08:38<00:00,  1.40it/s]\n",
            "100%|██████████| 883/883 [10:43<00:00,  1.37it/s]\n",
            "100%|██████████| 792/792 [09:56<00:00,  1.33it/s]\n",
            "100%|██████████| 856/856 [11:18<00:00,  1.26it/s]\n",
            "100%|██████████| 862/862 [11:09<00:00,  1.29it/s]\n",
            "100%|██████████| 958/958 [11:56<00:00,  1.34it/s]\n",
            "100%|██████████| 851/851 [10:53<00:00,  1.30it/s]\n",
            "100%|██████████| 923/923 [12:15<00:00,  1.25it/s]\n",
            "100%|██████████| 922/922 [10:48<00:00,  1.42it/s]\n",
            "100%|██████████| 428/428 [05:02<00:00,  1.41it/s]\n",
            "100%|██████████| 834/834 [10:17<00:00,  1.35it/s]\n",
            "100%|██████████| 600/600 [07:56<00:00,  1.26it/s]\n",
            " 14%|█▎        | 55/405 [00:42<04:34,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CUDA out of memory. Shifting inference to CPU for ID: 12560373056138365189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 405/405 [07:16<00:00,  1.08s/it]\n",
            "100%|██████████| 984/984 [11:58<00:00,  1.37it/s]\n",
            "100%|██████████| 761/761 [10:01<00:00,  1.26it/s]\n",
            "100%|██████████| 759/759 [09:22<00:00,  1.35it/s]\n",
            "100%|██████████| 512/512 [06:43<00:00,  1.27it/s]\n",
            "100%|██████████| 998/998 [14:30<00:00,  1.15it/s]\n",
            "100%|██████████| 478/478 [08:00<00:00,  1.00s/it]\n",
            "100%|██████████| 865/865 [11:54<00:00,  1.21it/s]\n",
            "100%|██████████| 973/973 [12:13<00:00,  1.33it/s]\n",
            "100%|██████████| 382/382 [04:34<00:00,  1.39it/s]\n",
            "100%|██████████| 723/723 [08:57<00:00,  1.35it/s]\n",
            "100%|██████████| 932/932 [11:28<00:00,  1.35it/s]\n",
            "100%|██████████| 927/927 [11:11<00:00,  1.38it/s]\n",
            "100%|██████████| 676/676 [08:19<00:00,  1.35it/s]\n",
            "100%|██████████| 883/883 [10:34<00:00,  1.39it/s]\n",
            "100%|██████████| 827/827 [13:04<00:00,  1.05it/s]\n",
            "100%|██████████| 949/949 [11:16<00:00,  1.40it/s]\n",
            "100%|██████████| 46/46 [00:39<00:00,  1.16it/s]\n",
            "100%|██████████| 541/541 [07:16<00:00,  1.24it/s]\n",
            "100%|██████████| 914/914 [11:13<00:00,  1.36it/s]\n",
            "100%|██████████| 775/775 [09:55<00:00,  1.30it/s]\n",
            "100%|██████████| 819/819 [10:04<00:00,  1.36it/s]\n",
            "100%|██████████| 256/256 [04:19<00:00,  1.01s/it]\n",
            "100%|██████████| 487/487 [06:12<00:00,  1.31it/s]\n",
            "100%|██████████| 758/758 [08:53<00:00,  1.42it/s]\n",
            "100%|██████████| 940/940 [11:49<00:00,  1.33it/s]\n",
            "100%|██████████| 986/986 [12:15<00:00,  1.34it/s]\n",
            "100%|██████████| 930/930 [11:23<00:00,  1.36it/s]\n",
            "100%|██████████| 41/41 [00:36<00:00,  1.11it/s]\n",
            "100%|██████████| 934/934 [11:19<00:00,  1.37it/s]\n",
            "100%|██████████| 418/418 [04:59<00:00,  1.40it/s]\n",
            "100%|██████████| 472/472 [05:40<00:00,  1.39it/s]\n",
            "100%|██████████| 700/700 [08:36<00:00,  1.36it/s]\n",
            "100%|██████████| 864/864 [10:51<00:00,  1.33it/s]\n",
            "100%|██████████| 967/967 [13:48<00:00,  1.17it/s]\n",
            "100%|██████████| 658/658 [07:56<00:00,  1.38it/s]\n",
            "100%|██████████| 790/790 [12:09<00:00,  1.08it/s]\n",
            "100%|██████████| 650/650 [08:22<00:00,  1.29it/s]\n",
            "100%|██████████| 650/650 [07:48<00:00,  1.39it/s]\n",
            "100%|██████████| 945/945 [11:24<00:00,  1.38it/s]\n",
            "100%|██████████| 925/925 [12:02<00:00,  1.28it/s]\n",
            " 66%|██████▌   | 554/842 [07:26<03:11,  1.51it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "for file_name in  x_eng_files:\n",
        "  lang_code = file_name.split(\"-\")[0].split(\"_\")[-1]\n",
        "  with open(base_path+file_name) as f:\n",
        "    ids=f.read().split()\n",
        "\n",
        "  if (lang_code in generated_translations.keys()) and (len(ids)==len(generated_translations[lang_code])) :\n",
        "    print(\"Done\")\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict[lang_code],split=\"test\",streaming=True,trust_remote_code=True)\n",
        "  except:\n",
        "    lang_issue.append(lang_code)\n",
        "    print(\"\\n Missing language \",lang_code)\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "  for item in tqdm(src_lang_data,total=len(ids)):\n",
        "    audio_sample = item['audio']\n",
        "    id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    if str(item['id'])+'_'+str(id) in generated_translations[lang_code]:\n",
        "      continue\n",
        "\n",
        "    if id not in ids:\n",
        "      continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Initially, try to process the audio on the GPU\n",
        "        audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\", sampling_rate=16000)\n",
        "        audio_inputs = {k: v.to('cuda') for k, v in audio_inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "        translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"\\nCUDA out of memory. Shifting inference to CPU for ID:\", id)\n",
        "            torch.cuda.empty_cache()  # Clear any unreleased memory\n",
        "\n",
        "            # Move the model to CPU for this inference\n",
        "            model.to('cpu')\n",
        "\n",
        "            try:\n",
        "                # Make sure audio_inputs are on the CPU as well\n",
        "                audio_inputs = {k: v.to('cpu') for k, v in audio_inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    output_tokens = model.generate(**audio_inputs, tgt_lang=\"eng\", generate_speech=False)\n",
        "                translation = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n",
        "            except Exception as cpu_e:\n",
        "                print(\"\\nFailed processing on CPU for ID:\", id, \"with error:\", cpu_e)\n",
        "                lang_missing_ids.append((lang_code, id))\n",
        "            finally:\n",
        "                # Regardless of the outcome, put the model back on the GPU for subsequent operations\n",
        "                model.to('cuda')\n",
        "        else:\n",
        "            print(\"\\nAn error occurred for ID:\", id, \"Error:\", e)\n",
        "            lang_missing_ids.append((lang_code, id))\n",
        "    except Exception as e:\n",
        "        print(\"\\nAn unexpected error occurred for ID:\", id, \"Error:\", e)\n",
        "        lang_missing_ids.append((lang_code, id))\n",
        "\n",
        "\n",
        "    del audio_inputs\n",
        "    del output_tokens\n",
        "\n",
        "    generated_translations[lang_code][str(item['id'])+'_'+str(id)]=translation\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  with open ('generated_large.json','w')as f:\n",
        "    json.dump(generated_translations,f,indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYrtUcIzFosB",
        "outputId": "81dfe5dd-e074-467c-d39b-514cd41163cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H40gXeodK-5F"
      },
      "outputs": [],
      "source": [
        "def get_bleu_score(generated_translations):\n",
        "\n",
        "  # get english data from fleurs dataset\n",
        "  eng_data=load_dataset('google/fleurs',name='en_us',trust_remote_code=True)\n",
        "  eng_translation={}\n",
        "  for split in eng_data:\n",
        "    for item in tqdm(eng_data[split]):\n",
        "      audio_sample = item['audio']\n",
        "      # id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "      eng_translation[item['id']]=item['raw_transcription']\n",
        "\n",
        "  bleu_score={}\n",
        "  for lang_code in list(generated_translations.keys()): #calculate bleu score for each language code\n",
        "\n",
        "    translations=[]\n",
        "    gt_translations=[]\n",
        "\n",
        "    for i in generated_translations[lang_code]:\n",
        "        key=int(i.split('_')[0])\n",
        "        gt_translations.append(eng_translation[key])\n",
        "        translations.append(generated_translations[lang_code][i])\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(translations, [gt_translations])\n",
        "    bleu_score[lang_code]=round(bleu.score, 3)\n",
        "\n",
        "  return bleu_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dr8J1tTJWr-a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('generated_large.json') as f:\n",
        "    generated_translations = json.load(f)\n",
        "\n",
        "# Convert to defaultdict with empty dictionaries as default values\n",
        "generated_translations = defaultdict(dict, generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQkxs_E8NKDs",
        "outputId": "54afd1c4-21da-4654-96cb-973f204183a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cc/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 2602/2602 [00:02<00:00, 926.92it/s]\n",
            "100%|██████████| 394/394 [00:00<00:00, 988.03it/s] \n",
            "100%|██████████| 647/647 [00:00<00:00, 954.91it/s]\n"
          ]
        }
      ],
      "source": [
        "seamless_fleurs_bleu=get_bleu_score(generated_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q-0FxJrXPv-",
        "outputId": "74ebb769-d2c8-4e92-db70-56765052e551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'afr': 39.69,\n",
              " 'amh': 17.034,\n",
              " 'arb': 31.725,\n",
              " 'asm': 17.47,\n",
              " 'ast': 25.894,\n",
              " 'azj': 16.425,\n",
              " 'bel': 16.056,\n",
              " 'ben': 22.778,\n",
              " 'bos': 32.891,\n",
              " 'bul': 31.33,\n",
              " 'cat': 37.574,\n",
              " 'ceb': 7.723,\n",
              " 'ces': 31.016,\n",
              " 'ckb': 20.487,\n",
              " 'cmn': 18.98,\n",
              " 'cym': 30.22,\n",
              " 'dan': 33.553,\n",
              " 'deu': 35.469,\n",
              " 'ell': 24.804,\n",
              " 'est': 28.534,\n",
              " 'fin': 25.782,\n",
              " 'fra': 32.641,\n",
              " 'ful': 0.788,\n",
              " 'gaz': 0.317,\n",
              " 'gle': 10.654,\n",
              " 'glg': 32.033,\n",
              " 'guj': 27.164,\n",
              " 'hau': 0.544,\n",
              " 'heb': 28.226,\n",
              " 'hin': 25.194,\n",
              " 'hrv': 29.8,\n",
              " 'hun': 24.166,\n",
              " 'hye': 27.81,\n",
              " 'ibo': 1.27,\n",
              " 'ind': 28.81,\n",
              " 'isl': 22.854,\n",
              " 'ita': 25.307,\n",
              " 'jav': 19.459,\n",
              " 'jpn': 15.886,\n",
              " 'kam': 1.803,\n",
              " 'kan': 21.799,\n",
              " 'kat': 18.741,\n",
              " 'kaz': 21.338,\n",
              " 'kea': 27.313,\n",
              " 'khk': 16.258,\n",
              " 'khm': 18.62,\n",
              " 'kir': 16.771,\n",
              " 'kor': 18.402,\n",
              " 'lao': 19.088,\n",
              " 'lin': 0.917,\n",
              " 'lit': 20.675,\n",
              " 'ltz': 14.429,\n",
              " 'lug': 16.179,\n",
              " 'luo': 0.789,\n",
              " 'lvs': 27.666,\n",
              " 'mal': 20.99,\n",
              " 'mar': 21.372,\n",
              " 'mkd': 33.972,\n",
              " 'mlt': 38.23,\n",
              " 'mri': 0.99,\n",
              " 'mya': 14.676,\n",
              " 'nld': 26.502,\n",
              " 'nob': 33.007,\n",
              " 'npi': 23.518,\n",
              " 'nso': 1.955,\n",
              " 'nya': 16.36,\n",
              " 'oci': 17.988,\n",
              " 'ory': 21.637,\n",
              " 'pan': 23.095,\n",
              " 'pbt': 11.242,\n",
              " 'pes': 27.74,\n",
              " 'pol': 22.163,\n",
              " 'por': 38.599,\n",
              " 'ron': 32.337,\n",
              " 'rus': 27.826,\n",
              " 'slk': 31.104,\n",
              " 'slv': 23.925,\n",
              " 'sna': 2.648,\n",
              " 'snd': 6.712,\n",
              " 'som': 14.259,\n",
              " 'spa': 24.544,\n",
              " 'srp': 35.158,\n",
              " 'swe': 33.979,\n",
              " 'swh': 25.754,\n",
              " 'tam': 15.136,\n",
              " 'tel': 20.319,\n",
              " 'tgk': 25.265,\n",
              " 'tgl': 21.896,\n",
              " 'tha': 18.013,\n",
              " 'tur': 24.819,\n",
              " 'ukr': 29.938,\n",
              " 'umb': 0.363,\n",
              " 'urd': 22.226,\n",
              " 'uzn': 21.475,\n",
              " 'vie': 20.481,\n",
              " 'wol': 0.832,\n",
              " 'xho': 3.126,\n",
              " 'yor': 12.487,\n",
              " 'yue': 12.381,\n",
              " 'zlm': 29.128,\n",
              " 'zul': 5.229}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(sorted(seamless_fleurs_bleu.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0i9Eat4-ldAe"
      },
      "outputs": [],
      "source": [
        "with open('seamless_large_fleurs_bleu.json','w')as f:\n",
        "  json.dump(seamless_fleurs_bleu,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59MfpGGyR464",
        "outputId": "af3d2620-dfcc-4538-fc62-e672522a5f83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
        "model.to('cuda')\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wJL2avl8cWSs"
      },
      "outputs": [],
      "source": [
        "allowed_whisper_lang=['en', 'zh', 'de', 'es', 'ru', 'ko', 'fr', 'ja', 'pt', 'tr', 'pl', 'ca', 'nl', 'ar', 'sv', 'it', 'id', 'hi', 'fi', 'vi', 'he', 'uk', 'el', 'ms', 'cs', 'ro', 'da', 'hu', 'ta', 'no', 'th', 'ur', 'hr', 'bg', 'lt', 'la', 'mi', 'ml', 'cy', 'sk', 'te', 'fa', 'lv', 'bn', 'sr', 'az', 'sl', 'kn', 'et', 'mk', 'br', 'eu', 'is', 'hy', 'ne', 'mn', 'bs', 'kk', 'sq', 'sw', 'gl', 'mr', 'pa', 'si', 'km', 'sn', 'yo', 'so', 'af', 'oc', 'ka', 'be', 'tg', 'sd', 'gu', 'am', 'yi', 'lo', 'uz', 'fo', 'ht', 'ps', 'tk', 'nn', 'mt', 'sa', 'lb', 'my', 'bo', 'tl', 'mg', 'as', 'tt', 'haw', 'ln', 'ha', 'ba', 'jw', 'su', 'yue', 'my', 'ca', 'nl', 'ht', 'lb', 'ps', 'pa', 'ro', 'ro', 'si', 'es', 'zh']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TrXHi9b3-xM_"
      },
      "outputs": [],
      "source": [
        "import pycountry\n",
        "\n",
        "def iso639_3_to_iso639_1(code_639_3):\n",
        "    # Cross-mapping dictionary from provided lists\n",
        "    cross_mapping = dict(zip(new_codes, old_codes))\n",
        "\n",
        "    # Additional special cases for direct conversion from ISO 639-3 to ISO 639-1\n",
        "    special_cases = {\n",
        "        'cmn': 'zh',  # Mandarin Chinese\n",
        "        'nob': 'no',  # Norwegian Bokmål\n",
        "        'jav': 'jw'   # Javanese\n",
        "        # Add any other special cases if needed\n",
        "    }\n",
        "\n",
        "    # Check special cases first\n",
        "    if code_639_3 in special_cases:\n",
        "        return special_cases[code_639_3]\n",
        "\n",
        "    # Use cross-mapping to find the corresponding old code if available\n",
        "    if code_639_3 in cross_mapping:\n",
        "        code_639_3 = cross_mapping[code_639_3]\n",
        "\n",
        "    # Use pycountry to attempt to convert any code to ISO 639-1 two-letter code\n",
        "    try:\n",
        "        language = pycountry.languages.get(alpha_3=code_639_3)\n",
        "        return language.alpha_2 if hasattr(language, 'alpha_2') else code_639_3\n",
        "    except AttributeError:\n",
        "        # Return the original code if no ISO 639-1 code is found\n",
        "        return code_639_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vyrMWWDW_o2d"
      },
      "outputs": [],
      "source": [
        "whisper_codes={}\n",
        "for code in lang_dict:\n",
        "  x=iso639_3_to_iso639_1(code)\n",
        "  if x in allowed_whisper_lang:\n",
        "    whisper_codes[code]=x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U8ksMLY_vDW",
        "outputId": "bfe66fb8-0608-4ea7-93c9-f99f37ec64ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(whisper_codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "EhPGic3Ngd0k"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "generated_transcriptions=collections.defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZNXqYAwLSxxN"
      },
      "outputs": [],
      "source": [
        "options = dict(beam_size=5, best_of=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IAoXwEn4TpOL"
      },
      "outputs": [],
      "source": [
        "lang_issue=[]\n",
        "lang_missing_ids=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJGFh4NA5lAg",
        "outputId": "753e2e68-4a02-40ae-bbe0-0592c09ed7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 958/958 [1:08:55<00:00,  4.32s/it]\n",
            "100%|██████████| 932/932 [1:01:08<00:00,  3.94s/it]\n",
            "100%|██████████| 883/883 [27:59<00:00,  1.90s/it]\n",
            "100%|██████████| 934/934 [31:05<00:00,  2.00s/it]\n",
            "100%|██████████| 792/792 [26:30<00:00,  2.01s/it]\n",
            "100%|██████████| 775/775 [23:39<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper ckb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 834/834 [26:26<00:00,  1.90s/it]\n",
            "100%|██████████| 905/905 [30:33<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper kir\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 743/743 [21:41<00:00,  1.75s/it]\n",
            "100%|██████████| 726/726 [41:45<00:00,  3.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper wol\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 945/945 [27:56<00:00,  1.77s/it]\n",
            "100%|██████████| 1015/1015 [58:49<00:00,  3.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Missing language in whisper ful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 67/851 [02:24<30:03,  2.30s/it]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "for file_name in  x_eng_files:\n",
        "  lang_code = file_name.split(\"-\")[0].split(\"_\")[-1]\n",
        "  with open(base_path+file_name) as f:\n",
        "    ids=f.read().split()\n",
        "\n",
        "  if (lang_code in generated_translations.keys()) and (len(ids)==len(generated_translations[lang_code])) :\n",
        "    print(\"Done\")\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "\n",
        "    forced_decoder_ids={\n",
        "    'translate' : processor.get_decoder_prompt_ids(language=whisper_codes[lang_code], task=\"translate\"),\n",
        "    'transcribe': processor.get_decoder_prompt_ids(language=whisper_codes[lang_code], task=\"transcribe\")\n",
        "    }\n",
        "    src_lang_data=load_dataset(\"google/fleurs\",name=lang_dict[lang_code],split=\"test\",streaming=True,trust_remote_code=True)\n",
        "\n",
        "  except:\n",
        "    lang_issue.append(lang_code)\n",
        "    print(\"\\n Missing language in whisper\",lang_code)\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for item in tqdm(src_lang_data,total=len(ids)):\n",
        "    audio_sample = item['audio']\n",
        "    id=audio_sample['path'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "    if str(item['id'])+'_'+str(id) in generated_translations[lang_code]:\n",
        "      continue\n",
        "\n",
        "    if id not in ids:\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "        # Initially, try to process the audio on the GPU\n",
        "        input_features = processor(audio_sample[\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "        input_features= input_features.to('cuda')\n",
        "        with torch.no_grad():\n",
        "          translate_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['translate'])\n",
        "          transcript_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['transcribe'])\n",
        "\n",
        "        translation = processor.batch_decode(translate_output, skip_special_tokens=True)\n",
        "        transcription = processor.batch_decode(transcript_output, skip_special_tokens=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"\\nCUDA out of memory. Shifting inference to CPU for ID:\", id)\n",
        "            torch.cuda.empty_cache()  # Clear any unreleased memory\n",
        "\n",
        "            # Move the model to CPU for this inference\n",
        "            model.to('cpu')\n",
        "\n",
        "            try:\n",
        "                input_features= input_features.to('cpu')\n",
        "                with torch.no_grad():\n",
        "                  translate_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['translate'])\n",
        "                  transcript_output = model.generate(input_features, forced_decoder_ids=forced_decoder_ids['transcribe'])\n",
        "\n",
        "                translation = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "                transcription = processor.batch_decode(transcript_output, skip_special_tokens=True)\n",
        "            except Exception as cpu_e:\n",
        "                print(\"\\nFailed processing on CPU for ID:\", id, \"with error:\", cpu_e)\n",
        "                lang_missing_ids.append((lang_code, id))\n",
        "            finally:\n",
        "                # Regardless of the outcome, put the model back on the GPU for subsequent operations\n",
        "                model.to('cuda')\n",
        "        else:\n",
        "            print(\"\\nAn error occurred for ID:\", id, \"Error:\", e)\n",
        "            lang_missing_ids.append((lang_code, id))\n",
        "    except Exception as e:\n",
        "        print(\"\\nAn unexpected error occurred for ID:\", id, \"Error:\", e)\n",
        "        lang_missing_ids.append((lang_code, id))\n",
        "\n",
        "    del input_features ,transcript_output, translate_output\n",
        "\n",
        "    generated_translations[lang_code][str(item['id'])+'_'+str(id)]=translation\n",
        "    generated_transcriptions[lang_code][str(item['id'])+'_'+str(id)]=transcription\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  with open ('generated_translations_whisper.json','w')as f:\n",
        "    json.dump(generated_translations,f,indent=2)\n",
        "\n",
        "  with open ('generated_transcriptions_whisper.json','w')as f:\n",
        "    json.dump(generated_transcriptions,f,indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9hATtvtxHY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPDwRAz7UWFOhO/04F9pdv+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
